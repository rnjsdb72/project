{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='tomato'><font color=\"#CC3D3D\"><p>\n",
    "# Build models using Keras Tuner and Winner's Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Image, clear_output\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib import font_manager, rc\n",
    "%matplotlib inline\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import shap\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import kerastuner as kt\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set random seeds to make your results reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 매번 모델링을 할 때마다 동일한 결과를 얻으려면 아래 코드를 실행해야 함.\n",
    "\n",
    "def reset_seeds(s1,s2,s3, reset_graph_with_backend=None):\n",
    "    if reset_graph_with_backend is not None:\n",
    "        K = reset_graph_with_backend\n",
    "        K.clear_session()\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        print(\"KERAS AND TENSORFLOW GRAPHS RESET\")  # optional\n",
    "\n",
    "    np.random.seed(s1)\n",
    "    random.seed(s2)\n",
    "    tf.compat.v1.set_random_seed(s3)\n",
    "#    os.environ['CUDA_VISIBLE_DEVICES'] = ''  # for GPU\n",
    "#    print(\"RANDOM SEEDS RESET\")  # optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load and process the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st round 1등 데이터\n",
    "w1_train = pd.DataFrame(pd.read_pickle(os.path.abspath(\"../input\")+'/1st_train_features.pkl'))\n",
    "w1_train.columns = ['w1_'+str(c) for c in w1_train.columns]\n",
    "w1_train['custid'] = pd.read_csv(os.path.abspath(\"../input\")+'/train_features_3rd_winner.csv').custid\n",
    "\n",
    "w1_test = pd.DataFrame(pd.read_pickle(os.path.abspath(\"../input\")+'/1st_test_features.pkl'))\n",
    "w1_test.columns = ['w1_'+str(c) for c in w1_test.columns]\n",
    "w1_test['custid'] = pd.read_csv(os.path.abspath(\"../input\")+'/test_features_3rd_winner.csv').custid\n",
    "\n",
    "# 1st round 2등 데이터\n",
    "train_x, valid_x, _, _, test_x, _ = pd.read_pickle(os.path.abspath(\"../input\")+'/2nd_data.pkl')\n",
    "w2_train = pd.DataFrame(np.vstack([train_x, valid_x]))\n",
    "w2_train.columns = ['w2_'+str(c) for c in w2_train.columns]\n",
    "w2_train['custid'] = w1_train['custid']\n",
    "\n",
    "w2_test = pd.DataFrame(test_x)\n",
    "w2_test.columns = ['w2_'+str(c) for c in w2_test.columns]\n",
    "w2_test['custid'] = w1_test['custid']\n",
    "\n",
    "# 1st round 3등 데이터\n",
    "w3_train = pd.read_csv(os.path.abspath(\"../input\")+'/train_features_3rd_winner.csv', index_col=0)\n",
    "w3_test = pd.read_csv(os.path.abspath(\"../input\")+'/test_features_3rd_winner.csv', index_col=0)\n",
    "w3_train.columns = ['custid']+['w3_'+str(c) for c in w3_train.columns[1:]]\n",
    "w3_test.columns = ['custid']+['w3_'+str(c) for c in w3_test.columns[1:]]\n",
    "\n",
    "# 1,2,3등 데이터 병합\n",
    "X_train = w1_train.merge(w2_train).merge(w3_train)\n",
    "X_test = w1_test.merge(w2_test).merge(w3_test)\n",
    "y_train = pd.read_csv(os.path.abspath(\"../input\")+'/y_train.csv').age\n",
    "IDtest = X_test.custid.unique()\n",
    "\n",
    "# Feature Selection: Using SHAP values\n",
    "IDtrain = X_train['custid']\n",
    "X_train = X_train.drop('custid', axis=1)\n",
    "X_test = X_test.drop('custid', axis=1)\n",
    "\n",
    "# DF, based on which importance is checked\n",
    "X_importance = X_test\n",
    "# Explain model predictions using shap library:\n",
    "model = LGBMRegressor(random_state=0).fit(X_train, y_train)\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_importance)\n",
    "shap_sum = np.abs(shap_values).mean(axis=0)\n",
    "importance_df = pd.DataFrame([X_importance.columns.tolist(), shap_sum.tolist()]).T\n",
    "importance_df.columns = ['column_name', 'shap_importance']\n",
    "importance_df = importance_df.sort_values('shap_importance', ascending=False)\n",
    "importance_df\n",
    "\n",
    "# 중요도가 0인 feature를 제거 \n",
    "features_selected = importance_df.query('shap_importance > 0').column_name\n",
    "X_train = X_train[features_selected]\n",
    "X_test = X_test[features_selected]\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_test[8978] = np.nan_to_num(X_test[8978], copy=False) # nan 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split data into train & validation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Validation\n",
    "i = int(round(X_train.shape[0] * 0.8,0))\n",
    "X_valid, y_valid = X_train[i:], y_train[i:]\n",
    "X_train, y_train = X_train[:i], y_train[:i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define the hyper-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(hp):\n",
    "    inputs = keras.Input(shape=(X_train.shape[1],))\n",
    "    x = inputs\n",
    "    act = hp.Choice('act', ['relu','elu','selu'])\n",
    "#    reg = hp.Float('reg', 0, 0.01, 0.01)\n",
    "    for i in range(hp.Int('num_layers', 2, 4)):\n",
    "        x = keras.layers.Dense(hp.Choice('unit_'+str(i), [512, 256, 128, 64, 32, 16]),\n",
    "#                               kernel_regularizer=keras.regularizers.l2(reg),\n",
    "                               activation=act)(x)\n",
    "        x = keras.layers.Dropout(hp.Float('dropout_'+str(i), 0, 0.5, step=0.25, default=0.5))(x)\n",
    "    outputs = keras.layers.Dense(1, activation='linear')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(loss='mse', \n",
    "#                  optimizer=tf.keras.optimizers.Adam(hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])), \n",
    "                  optimizer=hp.Choice('optimizer', ['adam','nadam', 'rmsprop']), \n",
    "                  metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build multiple hyper-tuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "preds = []\n",
    "\n",
    "for i in tqdm(range(N)):\n",
    "    reset_seeds(i,i*10,i*100)\n",
    "    tuner = kt.Hyperband(model_fn,\n",
    "                     objective=kt.Objective('val_root_mean_squared_error', direction=\"min\"), \n",
    "                     max_epochs=10,\n",
    "                     hyperband_iterations=2,\n",
    "                     overwrite=True,\n",
    "                     directory='dnn_tuning')\n",
    "    tuner.search(X_train, y_train, validation_data=(X_valid, y_valid), \n",
    "             callbacks=[tf.keras.callbacks.EarlyStopping(patience=2)], verbose=0)\n",
    "    model = tuner.get_best_models(1)[0]\n",
    "    preds.append(model.predict(X_test).flatten())         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Ensemble models & make submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Power mean ensemble\n",
    "p = 1\n",
    "pred = 0\n",
    "n = 0\n",
    "for i in range(N):\n",
    "    pred = pred + preds[i]**p \n",
    "    n += 1\n",
    "pred = pred / n    \n",
    "pred = pred**(1/p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make submissions: (Public LB) 8.15961\n",
    "t = pd.Timestamp.now()\n",
    "fname = f\"dnn_submission_{t.month:02}{t.day:02}{t.hour:02}{t.minute:02}.csv\"\n",
    "pd.DataFrame({'custid': IDtest, 'age': pred}).to_csv(fname, index=False)\n",
    "print(f\"'{fname}' is ready to submit.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#CC3D3D\"><p>\n",
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
