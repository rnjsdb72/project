{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba61cc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "plt.rc('font',family='malgun gothic')\n",
    "plt.rc('axes',unicode_minus=False)\n",
    "import seaborn as sns\n",
    "\n",
    "# EDA\n",
    "import klib\n",
    "\n",
    "# Preprocessing&Feature Engineering\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PowerTransformer, RobustScaler, MaxAbsScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from bayes_opt import BayesianOptimization\n",
    "import kerastuner as kt\n",
    "\n",
    "# Modeling\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, ARDRegression, BayesianRidge, Lars\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import VotingRegressor, StackingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import tensorflow as tf\n",
    "from vecstack import StackingTransformer\n",
    "from vecstack import stacking\n",
    "\n",
    "# Eveluation\n",
    "from sklearn.metrics import mean_squared_error # squared=False시 RMSE\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Utility\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "from IPython.display import Image\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import platform\n",
    "from itertools import combinations\n",
    "from scipy.stats.mstats import gmean\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c310992",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv(os.path.abspath(\"../input\")+'/x_train_raw.csv', index_col=0)\n",
    "x_test = pd.read_csv(os.path.abspath(\"../input\")+'/x_test_raw.csv', index_col=0)\n",
    "df_train = pd.read_csv(os.path.abspath(\"../input\")+'/X_train.csv', encoding='cp949')\n",
    "y_train = pd.read_csv(os.path.abspath(\"../input\")+'/y_train.csv', encoding='cp949').set_index('custid')\n",
    "df_test = pd.read_csv(os.path.abspath(\"../input\")+'/X_test.csv', encoding='cp949')\n",
    "test_id = df_test['custid'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3714138",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric = ['총구매액', '평균구매액', '최고구매금액', '총환불금액', '총환불횟수', '총할인금액', '평균할인금액', '총실구매액', '평균실구매액',\n",
    " '수입상품구매건수', '1월방문', '2월방문', '3월방문', '4월방문', '5월방문', '6월방문', '7월방문', '8월방문', '9월방문', '10월방문',\n",
    " '11월방문', '12월방문', '1_x', '2_x', '3_x', '4_x', '5_x', '6_x', '7_x', '8_x', '9_x', '10_x', '11_x', '12_x', '1_y', '2_y', '3_y',\n",
    " '4_y', '5_y', '6_y', '7_y', '8_y', '9_y', '10_y', '11_y', '12_y', '1_x.1', '2_x.1', '3_x.1', '4_x.1', '5_x.1', '6_x.1', '7_x.1',\n",
    " '8_x.1', '9_x.1', '10_x.1', '11_x.1', '12_x.1', '1_y.1', '2_y.1', '3_y.1', '4_y.1', '5_y.1', '6_y.1', '7_y.1', '8_y.1', '9_y.1',\n",
    " '10_y.1', '11_y.1', '12_y.1', '1_x.2', '2_x.2', '3_x.2', '4_x.2', '5_x.2', '6_x.2', '7_x.2', '8_x.2', '9_x.2', '10_x.2', '11_x.2',\n",
    " '12_x.2', '1_y.2', '2_y.2', '3_y.2', '4_y.2', '5_y.2', '6_y.2', '7_y.2', '8_y.2', '9_y.2', '10_y.2', '11_y.2', '12_y.2', '1_x.3',\n",
    " '2_x.3', '3_x.3', '4_x.3', '5_x.3', '6_x.3', '7_x.3', '8_x.3', '9_x.3', '10_x.3', '11_x.3', '12_x.3', '1월_구매상품다양성', '2월_구매상품다양성',\n",
    " '3월_구매상품다양성', '4월_구매상품다양성', '5월_구매상품다양성', '6월_구매상품다양성', '7월_구매상품다양성', '8월_구매상품다양성',\n",
    " '9월_구매상품다양성', '10월_구매상품다양성', '11월_구매상품다양성', '12월_구매상품다양성', '1월_구매상품개수', '2월_구매상품개수',\n",
    " '3월_구매상품개수', '4월_구매상품개수', '5월_구매상품개수', '6월_구매상품개수', '7월_구매상품개수', '8월_구매상품개수', '9월_구매상품개수',\n",
    " '10월_구매상품개수', '11월_구매상품개수', '12월_구매상품개수', '가을_x', '겨울_x', '봄_x', '여름_x', '가을_y', '겨울_y', '봄_y', '여름_y',\n",
    " '가을_x.1', '겨울_x.1', '봄_x.1', '여름_x.1', '가을_y.1', '겨울_y.1', '봄_y.1', '여름_y.1', '가을_x.2', '겨울_x.2', '봄_x.2', '여름_x.2',\n",
    " '가을_y.2', '겨울_y.2', '봄_y.2', '여름_y.2', '가을_x.3', '겨울_x.3', '봄_x.3', '여름_x.3', '가을_y.3', '겨울_y.3', '봄_y.3', '여름_y.3',\n",
    " '가을_구매상품다양성', '겨울_구매상품다양성', '봄_구매상품다양성', '여름_구매상품다양성', '가을_구매상품개수', '겨울_구매상품개수',\n",
    " '봄_구매상품개수', '여름_구매상품개수', '전반기_x', '후반기_x', '전반기_y', '후반기_y', '전반기_x.1', '후반기_x.1', '전반기_y.1',\n",
    " '후반기_y.1', '전반기_x.2', '후반기_x.2', '전반기_y.2', '후반기_y.2', '전반기_x.3', '후반기_x.3', '전반기_y.3', '후반기_y.3', '전반기_구매상품다양성',\n",
    " '후반기_구매상품다양성', '전반기_구매상품개수', '후반기_구매상품개수', '비성수기_x', '성수기_x', '비성수기_y', '성수기_y', '비성수기_x.1',\n",
    " '성수기_x.1', '비성수기_y.1', '성수기_y.1', '비성수기_x.2', '성수기_x.2', '비성수기_y.2', '성수기_y.2', '비성수기_x.3', '성수기_x.3',\n",
    " '비성수기_y.3', '성수기_y.3', '비성수기_구매상품다양성', '성수기_구매상품다양성', '비성수기_구매상품개수', '성수기_구매상품개수',\n",
    " '구매일수', '구매주기', '내점당구매금액', '내점당구매개수', '월말_x', '월중_x', '월초_x', '월말_y', '월중_y', '월초_y', '월말_x.1',\n",
    " '월중_x.1', '월초_x.1', '월말_y.1', '월중_y.1', '월초_y.1', '월말_x.2', '월중_x.2', '월초_x.2', '월말_y.2', '월중_y.2', '월초_y.2',\n",
    " '월말_x.3', '월중_x.3', '월초_x.3', '월말_y.3', '월중_y.3', '월초_y.3', '월말_구매상품다양성', '월중_구매상품다양성', '월초_구매상품다양성',\n",
    " '월말_구매상품개수', '월중_구매상품개수', '월초_구매상품개수', '월_x', '화_x', '수_x', '목_x', '금_x', '토_x', '일_x', '총방문횟수',\n",
    " '금_y', '목_y', '수_y', '월_y', '일_y', '토_y', '화_y', '금_x.1', '목_x.1', '수_x.1', '월_x.1', '일_x.1', '토_x.1', '화_x.1', '금_y.1',\n",
    " '목_y.1', '수_y.1', '월_y.1', '일_y.1', '토_y.1', '화_y.1', '금_x.2', '목_x.2', '수_x.2', '월_x.2', '일_x.2', '토_x.2', '화_x.2', '금_y.2',\n",
    " '목_y.2', '수_y.2', '월_y.2', '일_y.2', '토_y.2', '화_y.2','금_x.3', '목_x.3', '수_x.3', '월_x.3', '일_x.3', '토_x.3', '화_x.3', '금_y.3',\n",
    " '목_y.3', '수_y.3', '월_y.3', '일_y.3', '토_y.3', '화_y.3', '금_구매상품다양성', '목_구매상품다양성', '수_구매상품다양성', '월_구매상품다양성',\n",
    " '일_구매상품다양성', '토_구매상품다양성', '화_구매상품다양성', '금_구매상품개수', '목_구매상품개수', '수_구매상품개수', '월_구매상품개수',\n",
    " '일_구매상품개수', '토_구매상품개수', '화_구매상품개수', '주말_x', '주중_x', '주말_y', '주중_y', '주말_x.1', '주중_x.1', '주말_y.1',\n",
    " '주중_y.1', '주말_x.2', '주중_x.2', '주말_y.2', '주중_y.2', '주말_x.3', '주중_x.3', '주말_y.3', '주중_y.3', '주말_구매상품다양성',\n",
    " '주중_구매상품다양성', '주말_구매상품개수', '주중_구매상품개수', '9시방문', '10시방문', '11시방문', '12시방문', '13시방문', '14시방문',\n",
    " '15시방문', '16시방문', '17시방문', '18시방문', '19시방문', '20시방문', '21시방문', '22시방문', '9_y.3', '10_y.3', '11_y.3', '12_y.3',\n",
    " '13_x', '14_x', '15_x', '16_x', '17_x', '18_x', '19_x', '20_x', '21_x', '22_x', '9_x.4', '10_x.4', '11_x.4', '12_x.4', '13_y', '14_y',\n",
    " '15_y', '16_y', '17_y', '18_y', '19_y', '20_y', '21_y', '22_y', '9_y.4', '10_y.4', '11_y.4', '12_y.4', '13_x.1', '14_x.1', '15_x.1',\n",
    " '16_x.1', '17_x.1', '18_x.1', '19_x.1', '20_x.1', '21_x.1', '9_x.5', '10_x.5', '11_x.5', '12_x.5', '13_y.1', '14_y.1', '15_y.1', '16_y.1',\n",
    " '17_y.1', '18_y.1', '19_y.1', '20_y.1', '21_y.1', '22_x.1', '9_y.5', '10_y.5', '11_y.5', '12_y.5', '13_x.2', '14_x.2', '15_x.2',\n",
    " '16_x.2', '17_x.2', '18_x.2', '19_x.2', '20_x.2', '21_x.2', '22_y.1', '9_x.6', '10_x.6', '11_x.6', '12_x.6', '13_y.2', '14_y.2',\n",
    " '15_y.2', '16_y.2', '17_y.2', '18_y.2', '19_y.2', '20_y.2', '21_y.2', '22_x.2', '9_y.6', '10_y.6', '11_y.6', '12_y.6', '13_x.3',\n",
    " '14_x.3', '15_x.3', '16_x.3', '17_x.3', '18_x.3', '19_x.3', '20_x.3', '21_x.3', '22_y.2', '9시_구매상품다양성', '10시_구매상품다양성',\n",
    " '11시_구매상품다양성', '12시_구매상품다양성', '13시_구매상품다양성', '14시_구매상품다양성', '15시_구매상품다양성', '16시_구매상품다양성',\n",
    " '17시_구매상품다양성', '18시_구매상품다양성', '19시_구매상품다양성', '20시_구매상품다양성', '21시_구매상품다양성', '22시_구매상품다양성',\n",
    " '9시_구매상품개수', '10시_구매상품개수', '11시_구매상품개수', '12시_구매상품개수', '13시_구매상품개수', '14시_구매상품개수', '15시_구매상품개수',\n",
    " '16시_구매상품개수', '17시_구매상품개수', '18시_구매상품개수', '19시_구매상품개수', '20시_구매상품개수', '21시_구매상품개수', '22시_구매상품개수',\n",
    " '오전_x', '오후_x', '오전_y', '오후_y', '오전_x.1', '오후_x.1', '오전_y.1', '오후_y.1', '오전_x.2', '오후_x.2', '오전_y.2', '오후_y.2',\n",
    " '오전_x.3', '오후_x.3', '오전_y.3', '오후_y.3', '오전_구매상품다양성', '오후_구매상품다양성', '오전_구매상품개수', '오후_구매상품개수',\n",
    " '아침_구매건수_x', '저녁_구매건수_x', '점심_구매건수_x', '아침_구매건수_y', '저녁_구매건수_y', '점심_구매건수_y', '아침_구매건수_x.1',\n",
    " '저녁_구매건수_x.1', '점심_구매건수_x.1', '아침_구매건수_y.1', '저녁_구매건수_y.1', '점심_구매건수_y.1', '아침_구매건수_x.2', '저녁_구매건수_x.2',\n",
    " '점심_구매건수_x.2', '아침_구매건수_y.2', '저녁_구매건수_y.2', '점심_구매건수_y.2', '아침_구매건수_x.3', '저녁_구매건수_x.3', '점심_구매건수_x.3',\n",
    " '아침_구매건수_y.3', '저녁_구매건수_y.3', '점심_구매건수_y.3', '아침_구매건수_구매상품다양성', '저녁_구매건수_구매상품다양성', '점심_구매건수_구매상품다양성',\n",
    " '아침_구매건수_구매상품개수', '저녁_구매건수_구매상품개수', '점심_구매건수_구매상품개수', '평균쇼핑시간', '무역점방문', '본점방문',\n",
    " '신촌점방문', '천호점방문', '무역점_x', '본점_x', '신촌점_x', '천호점_x', '무역점_y', '본점_y', '신촌점_y', '천호점_y', '무역점_x.1',\n",
    " '본점_x.1', '신촌점_x.1', '천호점_x.1', '무역점_y.1', '본점_y.1', '신촌점_y.1', '천호점_y.1', '무역점_x.2', '본점_x.2', '신촌점_x.2',\n",
    " '천호점_x.2', '무역점_y.2', '본점_y.2', '신촌점_y.2', '천호점_y.2', '무역점', '본점', '신촌점', '천호점', '무역점_구매상품다양성',\n",
    " '본점_구매상품다양성', '신촌점_구매상품다양성', '천호점_구매상품다양성', '무역점_구매상품개수', '본점_구매상품개수', '신촌점_구매상품개수',\n",
    " '천호점_구매상품개수', '방문지점수', '방문지점개수', '구매상품다양성', '구매상품수', '평균할부개월수', '무이자할부평균가격', '할부결제건수',\n",
    " '1_y.3', '2_y.3', '3_y.3', '4_y.3', '5_y.3', '6_y.3', '7_y.3', '8_y.3', '9_x.7', '10_x.7', '11_x.7', '12_x.7', '1_x.4', '2_x.4', '3_x.4',\n",
    " '4_x.4', '5_x.4', '6_x.4', '7_x.4', '8_x.4', '9_y.7', '10_y.7', '11_y.7', '12_y.7', '1_y.4', '2_y.4', '3_y.4', '4_y.4','4_y.4', '5_y.4',\n",
    " '6_y.4', '7_y.4', '8_y.4', '9_x.8', '10_x.8', '11_x.8', '12_x.8', '1_x.5', '2_x.5', '3_x.5', '4_x.5', '5_x.5', '6_x.5', '7_x.5',\n",
    " '8_x.5', '9_y.8', '10_y.8', '11_y.8', '12_y.8', '1_y.5', '2_y.5', '3_y.5', '4_y.5', '5_y.5', '6_y.5', '7_y.5', '8_y.5', '9_x.9',\n",
    " '10_x.9', '11_x.9', '12_x.9', '1_x.6', '2_x.6', '3_x.6', '4_x.6', '5_x.6', '6_x.6', '7_x.6', '8_x.6', '9_y.9', '10_y.9', '11_y.9',\n",
    " '12_y.9', '1_y.6', '2_y.6', '3_y.6', '4_y.6', '5_y.6', '6_y.6', '7_y.6', '8_y.6', '9_x.10', '10_x.10', '11_x.10', '12_x.10', '1_x.7',\n",
    " '2_x.7', '3_x.7', '4_x.7', '5_x.7', '6_x.7', '7_x.7', '8_x.7', '9_y.10', '10_y.10', '11_y.10', '12_y.10', '1할부개월_구매상품다양성',\n",
    " '2할부개월_구매상품다양성', '3할부개월_구매상품다양성', '4할부개월_구매상품다양성', '5할부개월_구매상품다양성', '6할부개월_구매상품다양성',\n",
    " '7할부개월_구매상품다양성', '8할부개월_구매상품다양성', '9할부개월_구매상품다양성', '10할부개월_구매상품다양성', '11할부개월_구매상품다양성',\n",
    " '12할부개월_구매상품다양성', '1할부개월_구매상품개수', '2할부개월_구매상품개수', '3할부개월_구매상품개수', '4할부개월_구매상품개수',\n",
    " '5할부개월_구매상품개수', '6할부개월_구매상품개수', '7할부개월_구매상품개수', '8할부개월_구매상품개수', '9할부개월_구매상품개수',\n",
    " '10할부개월_구매상품개수', '11할부개월_구매상품개수', '12할부개월_구매상품개수', '가정용품_구매건수', '가정용품파트_구매건수', '골프/유니캐쥬얼_구매건수',\n",
    " '공산품_구매건수', '공산품파트_구매건수', '남성의류_구매건수', '남성정장스포츠_구매건수', '로얄부띠끄_구매건수', '로얄부틱_구매건수', '명품잡화_구매건수',\n",
    " '생식품_구매건수', '생식품파트_구매건수', '스포츠캐주얼_구매건수', '스포츠캐쥬얼_구매건수', '아동_구매건수', '아동,스포츠_구매건수',\n",
    " '아동문화_구매건수', '여성의류파트_구매건수', '여성정장_구매건수', '여성캐주얼_구매건수', '여성캐쥬얼_구매건수', '영라이브_구매건수',\n",
    " '영어덜트캐쥬얼_구매건수', '영캐릭터_구매건수', '영플라자_구매건수', '인터넷백화점_구매건수', '잡화_구매건수', '잡화파트_구매건수',\n",
    " '케주얼,구두,아동_구매건수', '패션잡화_구매건수', '가구_구매건수', '가전_구매건수', '기타바이어_구매건수', '니트단품_구매건수', '도자기크리스탈_구매건수',\n",
    " '디자이너부띠끄_구매건수', '문화완구_구매건수', '생활용품_구매건수', '섬유_구매건수', '수입명품_구매건수', '스포츠_구매건수', '엘레강스캐주얼_구매건수',\n",
    " '영캐주얼_구매건수', '유니캐주얼_구매건수', '유아동복_구매건수', '일반식품_구매건수', '장신구_구매건수', '점외_구매건수', '정장셔츠_구매건수',\n",
    " '조리식품_구매건수', '조리욕실_구매건수', '청과곡물_구매건수', '침구수예_구매건수', '캐릭터캐주얼_구매건수', '타운모피_구매건수',\n",
    " '트래디셔널캐주얼_구매건수', '피혁A_구매건수', '피혁B_구매건수', '행사장(남성)_구매건수', '행사장(아동스포츠)_구매건수', '행사장(여성정장)_구매건수',\n",
    " '행사장(여성캐주얼)_구매건수', '행사장(여성캐쥬)_구매건수', '행사장(잡화)_구매건수', '화장품_구매건수', '최애브랜드구매횟수', '판매일',\n",
    " '가정용품_총구매액', '가정용품파트_총구매액', '골프/유니캐쥬얼_총구매액', '공산품_총구매액', '공산품파트_총구매액', '남성의류_총구매액',\n",
    " '남성정장스포츠_총구매액', '로얄부띠끄_총구매액', '로얄부틱_총구매액', '명품잡화_총구매액', '생식품_총구매액', '생식품파트_총구매액',\n",
    " '스포츠캐주얼_총구매액', '스포츠캐쥬얼_총구매액', '아동_총구매액', '아동,스포츠_총구매액', '아동문화_총구매액', '여성의류파트_총구매액',\n",
    " '여성정장_총구매액', '여성캐주얼_총구매액', '여성캐쥬얼_총구매액', '영라이브_총구매액', '영어덜트캐쥬얼_총구매액', '영캐릭터_총구매액',\n",
    " '영플라자_총구매액', '인터넷백화점_총구매액', '잡화_총구매액', '잡화파트_총구매액', '케주얼,구두,아동_총구매액', '패션잡화_총구매액',\n",
    " '가정용품_평균구매액', '가정용품파트_평균구매액', '골프/유니캐쥬얼_평균구매액', '공산품_평균구매액', '공산품파트_평균구매액', '남성의류_평균구매액',\n",
    " '남성정장스포츠_평균구매액', '로얄부띠끄_평균구매액', '로얄부틱_평균구매액', '명품잡화_평균구매액', '생식품_평균구매액', '생식품파트_평균구매액',\n",
    " '스포츠캐주얼_평균구매액', '스포츠캐쥬얼_평균구매액', '아동_평균구매액', '아동,스포츠_평균구매액', '아동문화_평균구매액', '여성의류파트_평균구매액',\n",
    " '여성정장_평균구매액', '여성캐주얼_평균구매액', '여성캐쥬얼_평균구매액', '영라이브_평균구매액', '영어덜트캐쥬얼_평균구매액', '영캐릭터_평균구매액',\n",
    " '영플라자_평균구매액', '인터넷백화점_평균구매액', '잡화_평균구매액', '잡화파트_평균구매액', '케주얼,구두,아동_평균구매액', '패션잡화_평균구매액',\n",
    " '가정용품_할인금액', '가정용품파트_할인금액', '골프/유니캐쥬얼_할인금액', '공산품_할인금액', '공산품파트_할인금액', '남성의류_할인금액',\n",
    " '남성정장스포츠_할인금액', '로얄부띠끄_할인금액', '로얄부틱_할인금액', '명품잡화_할인금액', '생식품_할인금액', '생식품파트_할인금액',\n",
    " '스포츠캐주얼_할인금액', '스포츠캐쥬얼_할인금액', '아동_할인금액', '아동,스포츠_할인금액', '아동문화_할인금액', '여성의류파트_할인금액',\n",
    " '여성정장_할인금액', '여성캐주얼_할인금액', '여성캐쥬얼_할인금액', '영라이브_할인금액', '영어덜트캐쥬얼_할인금액', '영캐릭터_할인금액',\n",
    " '영플라자_할인금액', '인터넷백화점_할인금액', '잡화_할인금액', '잡화파트_할인금액', '케주얼,구두,아동_할인금액', '패션잡화_할인금액',\n",
    " '가정용품_구매상품다양성', '가정용품파트_구매상품다양성', '골프/유니캐쥬얼_구매상품다양성', '공산품_구매상품다양성', '공산품파트_구매상품다양성',\n",
    " '남성의류_구매상품다양성', '남성정장스포츠_구매상품다양성', '로얄부띠끄_구매상품다양성', '로얄부틱_구매상품다양성', '명품잡화_구매상품다양성',\n",
    " '생식품_구매상품다양성', '생식품파트_구매상품다양성', '스포츠캐주얼_구매상품다양성', '스포츠캐쥬얼_구매상품다양성', '아동_구매상품다양성',\n",
    " '아동,스포츠_구매상품다양성', '아동문화_구매상품다양성', '여성의류파트_구매상품다양성', '여성정장_구매상품다양성', '여성캐주얼_구매상품다양성',\n",
    " '여성캐쥬얼_구매상품다양성', '영라이브_구매상품다양성', '영어덜트캐쥬얼_구매상품다양성', '영캐릭터_구매상품다양성', '영플라자_구매상품다양성',\n",
    " '인터넷백화점_구매상품다양성', '잡화_구매상품다양성', '잡화파트_구매상품다양성', '케주얼,구두,아동_구매상품다양성', '패션잡화_구매상품다양성',\n",
    " '가정용품_구매상품개수', '가정용품파트_구매상품개수', '골프/유니캐쥬얼_구매상품개수', '공산품_구매상품개수', '공산품파트_구매상품개수',\n",
    " '남성의류_구매상품개수', '남성정장스포츠_구매상품개수', '로얄부띠끄_구매상품개수', '로얄부틱_구매상품개수', '명품잡화_구매상품개수',\n",
    " '생식품_구매상품개수', '생식품파트_구매상품개수', '스포츠캐주얼_구매상품개수', '스포츠캐쥬얼_구매상품개수', '아동_구매상품개수',\n",
    " '아동,스포츠_구매상품개수', '아동문화_구매상품개수', '여성의류파트_구매상품개수', '여성정장_구매상품개수', '여성캐주얼_구매상품개수',\n",
    " '여성캐쥬얼_구매상품개수', '영라이브_구매상품개수', '영어덜트캐쥬얼_구매상품개수', '영캐릭터_구매상품개수', '영플라자_구매상품개수',\n",
    " '인터넷백화점_구매상품개수', '잡화_구매상품개수', '잡화파트_구매상품개수', '케주얼,구두,아동_구매상품개수', '패션잡화_구매상품개수']\n",
    "prop = ['수입상품구매비율', '1월방문_prop', '2월방문_prop', '3월방문_prop', '4월방문_prop', '5월방문_prop', '6월방문_prop', '7월방문_prop',\n",
    " '8월방문_prop', '9월방문_prop', '10월방문_prop', '11월방문_prop', '12월방문_prop', '성수기방문비율', '비성수기방문비율', '전반기방문비율',\n",
    " '후반기방문비율', '가을_prop', '겨울_prop', '봄_prop', '여름_prop', '월_prop', '화_prop', '수_prop', '목_prop', '금_prop', '토_prop',\n",
    " '일_prop', '주말방문비율', '9시방문_prop', '10시방문_prop', '11시방문_prop', '12시방문_prop', '13시방문_prop', '14시방문_prop', '15시방문_prop',\n",
    " '16시방문_prop', '17시방문_prop', '18시방문_prop', '19시방문_prop', '20시방문_prop', '21시방문_prop', '22시방문_prop', '아침_구매건수_prop',\n",
    " '저녁_구매건수_prop', '점심_구매건수_prop', '오전방문비율', '오후방문비율', '무역점방문_prop', '본점방문_prop', '신촌점방문_prop',\n",
    " '천호점방문_prop', '할부결제비율', '할인율_x', '할인율_y',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80de6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_num = x_train[numeric+prop]\n",
    "x_train_cat = x_train.drop(columns=numeric, axis=1)\n",
    "x_test_num = x_test[numeric+prop]\n",
    "x_test_cat = x_test.drop(columns=numeric, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89fbafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train_num.shape, x_test_num.shape,'\\n',x_train_cat.shape,x_test_cat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b20e8d",
   "metadata": {},
   "source": [
    "## Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac55db74",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0310535",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_cat = x_train_cat\n",
    "# 사용할 모델 설정 (속도가 빠른 모델 사용 권장)\n",
    "model = Ridge(random_state=0)\n",
    "\n",
    "# 각 특성과 타깃(class) 사이에 유의한 통계적 관계가 있는지 계산하여 특성을 선택하는 방법 \n",
    "# feature 개수 바꿔가며 성능 test한다.\n",
    "cv_scores = []\n",
    "for p in tqdm(range(5,100,1)):\n",
    "    X_new = SelectPercentile(percentile=p).fit_transform(x_train_cat, y_train)    \n",
    "    cv_score = cross_val_score(model, X_new, y_train, scoring='neg_mean_squared_error', cv=5).mean()\n",
    "    cv_scores.append((p,cv_score))\n",
    "\n",
    "# Print the best percentile\n",
    "best_score = cv_scores[np.argmax([score for _, score in cv_scores])]\n",
    "print(best_score)\n",
    "\n",
    "# Plot the performance change with p\n",
    "plt.plot([k for k, _ in cv_scores], [score for _, score in cv_scores])\n",
    "plt.xlabel('Percent of features')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e3723d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 과적합을 피하기 위해 최적의 p값 주변의 값을 선택하는게 더 나은 결과를 얻을 수 있다. \n",
    "fs = SelectPercentile(percentile=best_score[0]).fit(x_train_cat, y_train)\n",
    "x_train_cat = fs.transform(x_train_cat)\n",
    "x_test_cat = fs.transform(x_test_cat)\n",
    "\n",
    "print(x_train_cat.shape)\n",
    "print(features_cat.columns[fs.get_support()].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0b317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(x_train_cat,columns=features_cat.columns[fs.get_support()].tolist()).to_csv('x_train_cat.csv')\n",
    "pd.DataFrame(x_test_cat,columns=features_cat.columns[fs.get_support()].tolist()).to_csv('x_test_cat.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77962432",
   "metadata": {},
   "source": [
    "### Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c217c873",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_cat = np.array(x_train_cat)\n",
    "x_test_cat = np.array(x_test_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343434f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_cat2, x_dev_cat, y_train2, y_dev = train_test_split(x_train_cat, y_train, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3716c857",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pbounds = {\n",
    "    'alpha':(0,50)\n",
    "}\n",
    "def rid_opt(alpha):\n",
    "    params = {\n",
    "        'alpha':alpha\n",
    "    }\n",
    "    rid = Ridge(random_state=0, **params)\n",
    "    rid.fit(x_train_cat2,y_train2)\n",
    "    score = mean_squared_error(rid.predict(x_dev_cat),y_dev,squared=False)\n",
    "    return -score\n",
    "BO_rid = BayesianOptimization(rid_opt, pbounds, random_state=0)\n",
    "BO_rid.maximize(init_points=50, n_iter=50) # init_points: exploration, n_iter: iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cf744f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BO_rid.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b25a3fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pbounds = {\n",
    "    'alpha':(0,50)\n",
    "}\n",
    "def las_opt(alpha):\n",
    "    params = {\n",
    "        'alpha':alpha\n",
    "    }\n",
    "    las = Lasso(random_state=0, **params)\n",
    "    las.fit(x_train_cat2,y_train2)\n",
    "    score = mean_squared_error(las.predict(x_dev_cat),y_dev,squared=False)\n",
    "    return -score\n",
    "BO_las = BayesianOptimization(las_opt, pbounds, random_state=0)\n",
    "BO_las.maximize(init_points=50, n_iter=50) # init_points: exploration, n_iter: iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d012746",
   "metadata": {},
   "outputs": [],
   "source": [
    "BO_las.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d417e56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pbounds = {\n",
    "    'alpha':(0,50)\n",
    "}\n",
    "def ela_opt(alpha):\n",
    "    params = {\n",
    "        'alpha':alpha\n",
    "    }\n",
    "    ela = ElasticNet(random_state=0, **params)\n",
    "    ela.fit(x_train_cat2,y_train2)\n",
    "    score = mean_squared_error(ela.predict(x_dev_cat),y_dev,squared=False)\n",
    "    return -score\n",
    "BO_ela = BayesianOptimization(ela_opt, pbounds, random_state=0)\n",
    "BO_ela.maximize(init_points=50, n_iter=50) # init_points: exploration, n_iter: iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179fdacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BO_ela.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c0c930",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pbounds = {\n",
    "    'n_iter':(100,1000),\n",
    "    'alpha_1':(0,50),\n",
    "    'alpha_2':(0,50),\n",
    "    'lambda_1':(0,10),\n",
    "    'lambda_2':(0,10)\n",
    "}\n",
    "def ard_opt(n_iter,alpha_1,alpha_2,lambda_1,lambda_2):\n",
    "    params = {\n",
    "        'n_iter':int(round(n_iter)),\n",
    "        'alpha_1':alpha_1,\n",
    "        'alpha_2':alpha_2,\n",
    "        'lambda_1':lambda_1,\n",
    "        'lambda_2':lambda_2\n",
    "    }\n",
    "    ard = ARDRegression(**params)\n",
    "    ard.fit(x_train_cat2,y_train2)\n",
    "    score = mean_squared_error(ard.predict(x_dev_cat),y_dev,squared=False)\n",
    "    return -score\n",
    "BO_ard = BayesianOptimization(ard_opt, pbounds, random_state=0)\n",
    "BO_ard.maximize(init_points=50, n_iter=50) # init_points: exploration, n_iter: iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43942d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BO_ard.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac542d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pbounds = {\n",
    "    'n_iter':(100,1000),\n",
    "    'alpha_1':(0,50),\n",
    "    'alpha_2':(0,50),\n",
    "    'lambda_1':(0,10),\n",
    "    'lambda_2':(0,10)\n",
    "}\n",
    "def bay_opt(n_iter,alpha_1,alpha_2,lambda_1,lambda_2):\n",
    "    params = {\n",
    "        'n_iter':int(round(n_iter)),\n",
    "        'alpha_1':alpha_1,\n",
    "        'alpha_2':alpha_2,\n",
    "        'lambda_1':lambda_1,\n",
    "        'lambda_2':lambda_2\n",
    "    }\n",
    "    bay = BayesianRidge(**params)\n",
    "    bay.fit(x_train_cat2,y_train2)\n",
    "    score = mean_squared_error(bay.predict(x_dev_cat),y_dev,squared=False)\n",
    "    return -score\n",
    "BO_bay = BayesianOptimization(bay_opt, pbounds, random_state=0)\n",
    "BO_bay.maximize(init_points=50, n_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2a8fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BO_bay.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb53c56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pbounds = {\n",
    "    'n_estimators':(100,1000),\n",
    "    'learning_rate':(0,1),\n",
    "    'max_depth':(2, 32),\n",
    "    'num_leaves':(2, 64),\n",
    "    'min_child_samples':(10, 200),\n",
    "    'min_child_weight':(1, 50),\n",
    "    'subsample':(0.5, 1),\n",
    "    'colsample_bytree':(0.5, 1),\n",
    "    'max_bin':(10, 500),\n",
    "    'reg_lambda':(0.001, 10),\n",
    "    'reg_alpha':(0.01, 50)\n",
    "}\n",
    "def lgbm_opt(n_estimators, learning_rate, max_depth, num_leaves, min_child_samples, min_child_weight,\n",
    "             subsample, colsample_bytree, max_bin, reg_lambda, reg_alpha):\n",
    "    params = {\n",
    "        \"n_estimators\":int(round(n_estimators)), \n",
    "        \"learning_rate\":learning_rate,\n",
    "        'max_depth':int(round(max_depth)),\n",
    "        'num_leaves':int(round(num_leaves)),\n",
    "        'min_child_samples': int(round(min_child_samples)),\n",
    "        'min_child_weight': int(round(min_child_weight)),\n",
    "        'subsample':max(min(subsample, 1), 0),\n",
    "        'colsample_bytree':max(min(colsample_bytree, 1), 0),\n",
    "        'reg_lambda': reg_lambda,\n",
    "        'reg_alpha': reg_alpha\n",
    "    }\n",
    "    lgbm = LGBMRegressor(random_state=0, **params)\n",
    "    lgbm.fit(x_train_cat2,y_train2)\n",
    "    score = mean_squared_error(lgbm.predict(x_dev_cat),y_dev,squared=False)\n",
    "    return -score\n",
    "BO_lgbm = BayesianOptimization(lgbm_opt, pbounds, random_state=0)\n",
    "BO_lgbm.maximize(init_points=50, n_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05888d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BO_lgbm.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a25cb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_params_rid = BO_rid.max['params']\n",
    "max_params_las = BO_las.max['params']\n",
    "max_params_ela = BO_ela.max['params']\n",
    "max_params_ard = BO_ard.max['params']\n",
    "max_params_bay = BO_bay.max['params']\n",
    "max_params_lgbm = BO_lgbm.max['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1681553e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_params_ard['n_iter'] = int(round(max_params_ard['n_iter']))\n",
    "\n",
    "max_params_bay['n_iter'] = int(round(max_params_bay['n_iter']))\n",
    "\n",
    "max_params_lgbm['num_leaves'] = int(round(max_params_lgbm['num_leaves']))\n",
    "max_params_lgbm['n_estimators'] = int(round(max_params_lgbm['n_estimators']))\n",
    "max_params_lgbm['max_depth'] = int(round(max_params_lgbm['max_depth']))\n",
    "max_params_lgbm['min_child_samples'] = int(round(max_params_lgbm['min_child_samples']))\n",
    "max_params_lgbm['min_child_weight'] = int(round(max_params_lgbm['min_child_weight']))\n",
    "max_params_lgbm['max_bin'] = int(round(max_params_lgbm['max_bin']))\n",
    "max_params_lgbm['subsample'] = max(min(max_params_lgbm['subsample'], 1), 0)\n",
    "max_params_lgbm['colsample_bytree'] = max(min(max_params_lgbm['colsample_bytree'], 1), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04774cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_params_rid,'\\n',max_params_las,'\\n',max_params_ela,'\\n',max_params_ard,'\\n',max_params_bay,'\\n',max_params_lgbm,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a41edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_params_rid = {'alpha': 2.141450519257165} \n",
    "max_params_las = {'alpha': 0.0031679860402866744} \n",
    "max_params_ela = {'alpha': 0.0031679860402866744} \n",
    "max_params_ard = {'alpha_1': 34.76870111024178, 'alpha_2': 40.063650975282826, 'lambda_1': 0.6487969603784871, 'lambda_2': 9.930508557701192, 'n_iter': 774} \n",
    "max_params_bay = {'alpha_1': 49.03743486079745, 'alpha_2': 40.03692702558726, 'lambda_1': 0.0, 'lambda_2': 10.0, 'n_iter': 747} \n",
    "max_params_lgbm = {'colsample_bytree': 0.7842169744343243, 'learning_rate': 0.018789800436355142, 'max_bin': 313, 'max_depth': 20, 'min_child_samples': 127, 'min_child_weight': 47, 'n_estimators': 714, 'num_leaves': 24, 'reg_alpha': 21.857227370429083, 'reg_lambda': 6.976614328076722, 'subsample': 0.5301127358146349}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec81b0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "regs_tuned = [Ridge(random_state=0, **max_params_rid),Lasso(random_state=0, **max_params_las),ElasticNet(random_state=0, **max_params_ela),\n",
    "             ARDRegression(**max_params_ard),BayesianRidge(**max_params_bay),LGBMRegressor(random_state=0,**max_params_lgbm),CatBoostRegressor(random_state=0)]\n",
    "regs_tuned = [(str(reg).split('(')[0], reg) for reg in regs_tuned]\n",
    "regs_tuned[-1] = list(regs_tuned[-1])\n",
    "regs_tuned[-1][0] = 'CatBoostRegressor'\n",
    "regs_tuned[-1] = tuple(regs_tuned[-1])\n",
    "\n",
    "regs_trained = [(name, reg.fit(x_train_cat2,y_train2), float(mean_squared_error(reg.predict(x_dev_cat),y_dev,squared=False))) \n",
    "                    for name, reg in tqdm(regs_tuned.copy())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b177f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "regs_tuned = [Ridge(random_state=0, **max_params_rid),Lasso(random_state=0, **max_params_las),ElasticNet(random_state=0, **max_params_ela),\n",
    "             ARDRegression(**max_params_ard),BayesianRidge(**max_params_bay),LGBMRegressor(random_state=0,**max_params_lgbm),CatBoostRegressor(random_state=0)]\n",
    "regs_tuned = [(str(reg).split('(')[0], reg) for reg in regs_tuned]\n",
    "regs_tuned[-1] = list(regs_tuned[-1])\n",
    "regs_tuned[-1][0] = 'CatBoostRegressor'\n",
    "regs_tuned[-1] = tuple(regs_tuned[-1])\n",
    "\n",
    "regs_trained_for_submissions = [(name, reg.fit(x_train_cat,y_train)) for name, reg in tqdm(regs_tuned.copy())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598a879c",
   "metadata": {},
   "outputs": [],
   "source": [
    "regs_tuned = [Ridge(random_state=0, **max_params_rid),Lasso(random_state=0, **max_params_las),ElasticNet(random_state=0, **max_params_ela),\n",
    "             ARDRegression(**max_params_ard),BayesianRidge(**max_params_bay),LGBMRegressor(random_state=0,**max_params_lgbm),CatBoostRegressor(random_state=0)]\n",
    "regs_tuned = [(str(reg).split('(')[0], reg) for reg in regs_tuned]\n",
    "regs_tuned[-1] = list(regs_tuned[-1])\n",
    "regs_tuned[-1][0] = 'CatBoostRegressor'\n",
    "regs_tuned[-1] = tuple(regs_tuned[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca976435",
   "metadata": {},
   "source": [
    "### Model Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ee9f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results = []\n",
    "for name, reg, reg_score in regs_trained:\n",
    "    pred = list(reg.predict(x_dev_cat))\n",
    "    name = f'{name} \\n({reg_score:.4f})'\n",
    "    pred_results.append(pd.Series(pred, name=name))\n",
    "ensemble_results = pd.concat(pred_results, axis=1)\n",
    "ensemble_results = ensemble_results.applymap(lambda x: float(x))\n",
    "\n",
    "# 모형의 예측값 간의 상관관계를 보기 위해 hitmap을 도식한다.\n",
    "plt.figure(figsize = (8,6))\n",
    "g = sns.heatmap(ensemble_results.corr(), annot=True, cmap='Blues')\n",
    "g.set_title(\"Correlation between models\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d206c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = (ensemble_results.corr().sum()-1)/(ensemble_results.corr().shape[0]-1)\n",
    "names = corr.index\n",
    "rmse = np.array(corr.index.str[-7:-1]).astype(float)\n",
    "df = pd.DataFrame({'model': names, 'rmse': rmse, 'cor': corr})        \n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "g = sns.scatterplot(x=\"cor\", y=\"rmse\", data=df, s=40, color='red')\n",
    "for line in range(0, df.shape[0]):\n",
    "     g.text(df.cor[line]+0.003, df.rmse[line]-0.003, \n",
    "            df.model[line], horizontalalignment='left', \n",
    "            size='medium', color='black', weight='semibold')\n",
    "        \n",
    "plt.xlim((df.cor.min()-0.01,df.cor.max()+0.01))\n",
    "plt.ylim((df.rmse.min()-0.01,df.rmse.max()+0.01))\n",
    "plt.xlabel('Mean Agreement')\n",
    "plt.ylabel('RMSE')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034934ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = [#'LinearRegression',\n",
    "            'Ridge',\n",
    "            #'Lasso',\n",
    "            #'ElasticNet',\n",
    "            #'ARDRegression',\n",
    "            #'BayesianRidge',\n",
    "            #'RandomForestRegressor',\n",
    "            #'XGBRegressor',\n",
    "            'LGBMRegressor',\n",
    "            'CatBoostRegressor'\n",
    "            ]\n",
    "models_for_ensemble = [(name,reg) for name,reg,score in regs_trained if name in selected]\n",
    "results_for_ensemble = []\n",
    "for name, model in models_for_ensemble:\n",
    "    results_for_ensemble.append(model.predict(x_dev_cat))\n",
    "avg_test = (results_for_ensemble[0].flatten()+results_for_ensemble[1]+results_for_ensemble[2])/len(results_for_ensemble)\n",
    "score = mean_squared_error(avg_test, y_dev, squared=False)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7767144b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 최적의 가중치 찾기 \n",
    "selected = [#'LinearRegression',\n",
    "            'Ridge',\n",
    "            #'Lasso',\n",
    "            #'ElasticNet',\n",
    "            #'ARDRegression',\n",
    "            #'BayesianRidge',\n",
    "            #'RandomForestRegressor',\n",
    "            #'XGBRegressor',\n",
    "            'LGBMRegressor',\n",
    "            'CatBoostRegressor'\n",
    "            ]\n",
    "models_for_ensemble = [(name,reg) for name,reg,score in regs_trained if name in selected]\n",
    "weights_avg = []\n",
    "rmse_best = 1000\n",
    "for i in tqdm(range(1, 30, 1)):\n",
    "    for j in range(1, 30, 1):\n",
    "        for k in range(1, 30, 1):\n",
    "            if (i+j+k) != 30:\n",
    "                continue\n",
    "            pred = (models_for_ensemble[0][1].predict(x_dev_cat).flatten() * i + models_for_ensemble[1][1].predict(x_dev_cat) * j\n",
    "                    + models_for_ensemble[2][1].predict(x_dev_cat) * k)/30\n",
    "            rmse = np.sqrt(mean_squared_error(y_dev, pred))\n",
    "            if rmse < rmse_best:\n",
    "                weights_avg = [i,j,k]\n",
    "                rmse_best = rmse \n",
    "                print(rmse, i,j,k)            \n",
    "\n",
    "print(rmse_best, weights_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9608064",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_avg = [9,17,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d6f797",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = [#'LinearRegression',\n",
    "            'Ridge',\n",
    "            #'Lasso',\n",
    "            #'ElasticNet',\n",
    "            #'ARDRegression',\n",
    "            #'BayesianRidge',\n",
    "            #'RandomForestRegressor',\n",
    "            #'XGBRegressor',\n",
    "            'LGBMRegressor',\n",
    "            'CatBoostRegressor'\n",
    "            ]\n",
    "models_for_ensemble = [(name,reg) for name,reg,score in regs_trained if name in selected]\n",
    "i,j,k = weights_avg\n",
    "avg_test_result = (models_for_ensemble[0][1].predict(x_dev_cat).flatten()*i + models_for_ensemble[1][1].predict(x_dev_cat)*j + \n",
    "       models_for_ensemble[2][1].predict(x_dev_cat)*k)/30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f81e954",
   "metadata": {},
   "source": [
    "### Deep Neural Network\n",
    "'dnn_cat.krs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db11e39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_seeds(reset_graph_with_backend=None):\n",
    "    if reset_graph_with_backend is not None:\n",
    "        K = reset_graph_with_backend\n",
    "        K.clear_session()\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        print(\"KERAS AND TENSORFLOW GRAPHS RESET\")  # optional\n",
    "\n",
    "    np.random.seed(99)\n",
    "    # seed를 잘 설정하면 성능이 더 잘 오른다.\n",
    "    random.seed(9)\n",
    "    tf.compat.v1.set_random_seed(16)\n",
    "#    os.environ['CUDA_VISIBLE_DEVICES'] = ''  # for GPU\n",
    "    print(\"RANDOM SEEDS RESET\")  # optional\n",
    "   \n",
    "reset_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8defb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = int(round(x_train_cat2.shape[0] * 0.8,0))\n",
    "x_val_cat, y_val = x_train_cat2[i:], y_train2[i:]\n",
    "x_train_cat3, y_train3 = x_train_cat2[:i], y_train2[:i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b558aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def model_fn(hp):\n",
    "    inputs = tf.keras.Input(shape=(x_train_cat3.shape[1],))\n",
    "    x = inputs\n",
    "    for i in range(hp.Int('num_layers', 2, 4, step=1)):\n",
    "        x = tf.keras.layers.Dense(hp.Int('unit_'+str(i), 16, 256, step=16),\n",
    "                               activation=hp.Choice('activation',['relu','tanh']))(x)\n",
    "        x = tf.keras.layers.Dropout(hp.Float('dropout_'+str(i), 0, 0.7, step=0.1, default=0.5))(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(loss='mse', \n",
    "                  optimizer=tf.keras.optimizers.Adam(hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4, 1e-5, 1e-6])), \n",
    "                  metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model\n",
    "\n",
    "# keras tuner는 튜닝 종류가 4종류가 있음: hyperband, grid search, random search, bayesian optimization\n",
    "tuner = kt.Hyperband(model_fn,\n",
    "                     objective=kt.Objective('val_root_mean_squared_error', direction=\"min\"), \n",
    "                     max_epochs=30,\n",
    "                     hyperband_iterations=2,\n",
    "                     overwrite=True,\n",
    "                     directory='dnn_tuning')\n",
    "# objective: 튜닝 기준, hyperband_iterations:이거 자체에서 2번 반복\n",
    "# overwrite: False시, 기존을 근거로 해 재학습 안시킴\n",
    "\n",
    "tuner.search(x_train_cat3, y_train3, validation_data=(x_val_cat, y_val),\n",
    "             callbacks=[tf.keras.callbacks.EarlyStopping()])\n",
    "# 빨리 끝내려고 파라미터 저렇게 설정한 것임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998f6152",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary(1) # 1= 제일 성능이 좋은 놈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b6e670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss & RMSE\n",
    "dnn = tuner.get_best_models(1)[0] # best model 중 가장 좋은 모델\n",
    "dnn.evaluate(x_dev_cat, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d954082",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efded4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(pd.DataFrame(dict(zip(['dnn','avg'],[dnn.predict(x_dev_cat).flatten(),avg_test_result]))).corr(), annot=True, cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32d62a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_squared_error((dnn.predict(x_dev_cat).flatten()+avg_test_result)/2,y_dev,squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19704b5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weights_da = []\n",
    "rmse_best = 1000\n",
    "for i in tqdm(range(1,20)):\n",
    "    for j in range(1,20):\n",
    "        if i+j!=20:\n",
    "            continue\n",
    "        pred = (avg_test_result*i + dnn.predict(x_dev_cat).flatten()*j)/20\n",
    "        rmse = np.sqrt(mean_squared_error(y_dev, pred))\n",
    "        if rmse < rmse_best:\n",
    "            weights_da = [i,j]\n",
    "            rmse_best = rmse \n",
    "            print(rmse, i,j)     \n",
    "print(rmse_best, weights_da)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1d4982",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fc5d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = [#'LinearRegression',\n",
    "            'Ridge',\n",
    "            #'Lasso',\n",
    "            #'ElasticNet',\n",
    "            #'ARDRegression',\n",
    "            #'BayesianRidge',\n",
    "            #'RandomForestRegressor',\n",
    "            #'XGBRegressor',\n",
    "            'LGBMRegressor',\n",
    "            'CatBoostRegressor'\n",
    "            ]\n",
    "stack_estimators = [reg for name,reg,score in regs_trained if name in selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65b0310",
   "metadata": {},
   "outputs": [],
   "source": [
    "S_train, S_test = stacking(stack_estimators,\n",
    "                           x_train_cat, y_train, x_test_cat,\n",
    "                           regression=True, n_folds=5, stratified=True, shuffle=True,\n",
    "                           random_state=0, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704d9cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "S_train2, S_dev = stacking(stack_estimators,\n",
    "                           x_train_cat2, y_train2, x_dev_cat,\n",
    "                           regression=True, n_folds=5, stratified=True, shuffle=True,\n",
    "                           random_state=0, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362c1623",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pbounds = {\n",
    "    'alpha':(0,50)\n",
    "}\n",
    "def rid_stk_opt(alpha):\n",
    "    params = {\n",
    "        'alpha':alpha\n",
    "    }\n",
    "    rid = Ridge(random_state=0, **params)\n",
    "    skf = StratifiedKFold(n_splits=5 , shuffle=True, random_state=1)\n",
    "    score = cross_val_score(rid, S_train2, y_train2, scoring='neg_mean_squared_error', cv=skf, n_jobs=-1)\n",
    "    return -np.sqrt(-np.mean(score))\n",
    "BO_stk_rid = BayesianOptimization(rid_stk_opt, pbounds, random_state=0)\n",
    "BO_stk_rid.maximize(init_points=50, n_iter=50) # init_points: exploration, n_iter: iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de05efe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BO_stk_rid.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ffec61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pbounds = {\n",
    "    'alpha':(0,50)\n",
    "}\n",
    "def las_stk_opt(alpha):\n",
    "    params = {\n",
    "        'alpha':alpha\n",
    "    }\n",
    "    las = Lasso(random_state=0, **params)\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    score = cross_val_score(las,S_train2, y_train2, scoring='neg_mean_squared_error', cv=skf, n_jobs=-1)\n",
    "    return -np.sqrt(-np.mean(score))\n",
    "BO_stk_las = BayesianOptimization(las_stk_opt, pbounds, random_state=0)\n",
    "BO_stk_las.maximize(init_points=50, n_iter=50) # init_points: exploration, n_iter: iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0e4350",
   "metadata": {},
   "outputs": [],
   "source": [
    "BO_stk_las.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8558d9f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pbounds = {\n",
    "    'alpha':(0,50)\n",
    "}\n",
    "def ela_stk_opt(alpha):\n",
    "    params = {\n",
    "        'alpha':alpha\n",
    "    }\n",
    "    ela = ElasticNet(random_state=0, **params)\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    score = cross_val_score(ela, S_train2, y_train2, scoring='neg_mean_squared_error', cv=skf, n_jobs=-1)\n",
    "    return -np.sqrt(-np.mean(score))\n",
    "BO_stk_ela = BayesianOptimization(ela_stk_opt, pbounds, random_state=0)\n",
    "BO_stk_ela.maximize(init_points=50, n_iter=50) # init_points: exploration, n_iter: iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58427361",
   "metadata": {},
   "outputs": [],
   "source": [
    "BO_stk_ela.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4593bf43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pbounds = {\n",
    "    'n_iter':(100,1000),\n",
    "    'alpha_1':(0,50),\n",
    "    'alpha_2':(0,50),\n",
    "    'lambda_1':(0,10),\n",
    "    'lambda_2':(0,10)\n",
    "}\n",
    "def ard_stk_opt(n_iter,alpha_1,alpha_2,lambda_1,lambda_2):\n",
    "    params = {\n",
    "        'n_iter':int(round(n_iter)),\n",
    "        'alpha_1':alpha_1,\n",
    "        'alpha_2':alpha_2,\n",
    "        'lambda_1':lambda_1,\n",
    "        'lambda_2':lambda_2\n",
    "    }\n",
    "    ard = ARDRegression(**params)\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    score = cross_val_score(ard, S_train2, y_train2, scoring='neg_mean_squared_error', cv=skf, n_jobs=-1)\n",
    "    return -np.sqrt(-np.mean(score))\n",
    "BO_stk_ard = BayesianOptimization(ard_stk_opt, pbounds, random_state=0)\n",
    "BO_stk_ard.maximize(init_points=50, n_iter=50) # init_points: exploration, n_iter: iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49eb955",
   "metadata": {},
   "outputs": [],
   "source": [
    "BO_stk_ard.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60571c0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pbounds = {\n",
    "    'n_iter':(100,1000),\n",
    "    'alpha_1':(0,50),\n",
    "    'alpha_2':(0,50),\n",
    "    'lambda_1':(0,10),\n",
    "    'lambda_2':(0,10)\n",
    "}\n",
    "def bay_stk_opt(n_iter,alpha_1,alpha_2,lambda_1,lambda_2):\n",
    "    params = {\n",
    "        'n_iter':int(round(n_iter)),\n",
    "        'alpha_1':alpha_1,\n",
    "        'alpha_2':alpha_2,\n",
    "        'lambda_1':lambda_1,\n",
    "        'lambda_2':lambda_2\n",
    "    }\n",
    "    bay = BayesianRidge(**params)\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    score = cross_val_score(bay,S_train2, y_train2, scoring='neg_mean_squared_error', cv=skf, n_jobs=-1)\n",
    "    return -np.sqrt(-np.mean(score))\n",
    "BO_stk_bay = BayesianOptimization(bay_stk_opt, pbounds, random_state=0)\n",
    "BO_stk_bay.maximize(init_points=50, n_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2e4007",
   "metadata": {},
   "outputs": [],
   "source": [
    "BO_stk_bay.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ddd01a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pbounds = {\n",
    "    'n_estimators':(100,1000),\n",
    "    'learning_rate':(0.0000000000000000001,1),\n",
    "    'max_depth':(2, 32),\n",
    "    'num_leaves':(2, 64),\n",
    "    'min_child_samples':(10, 200),\n",
    "    'min_child_weight':(1, 50),\n",
    "    'subsample':(0.5, 1),\n",
    "    'colsample_bytree':(0.5, 1),\n",
    "    'max_bin':(10, 500),\n",
    "    'reg_lambda':(0.001, 10),\n",
    "    'reg_alpha':(0.01, 50)\n",
    "}\n",
    "def lgbm_stk_opt(n_estimators, learning_rate, max_depth, num_leaves, min_child_samples, min_child_weight,\n",
    "             subsample, colsample_bytree, max_bin, reg_lambda, reg_alpha):\n",
    "    params = {\n",
    "        \"n_estimators\":int(round(n_estimators)), \n",
    "        \"learning_rate\":learning_rate,\n",
    "        'max_depth':int(round(max_depth)),\n",
    "        'num_leaves':int(round(num_leaves)),\n",
    "        'min_child_samples': int(round(min_child_samples)),\n",
    "        'min_child_weight': int(round(min_child_weight)),\n",
    "        'subsample':max(min(subsample, 1), 0),\n",
    "        'colsample_bytree':max(min(colsample_bytree, 1), 0),\n",
    "        'reg_lambda': reg_lambda,\n",
    "        'reg_alpha': reg_alpha\n",
    "    }\n",
    "    lgbm = LGBMRegressor(random_state=0, **params)\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    score = cross_val_score(lgbm, S_train2, y_train2, scoring='neg_mean_squared_error', cv=skf, n_jobs=-1)\n",
    "    return -np.sqrt(-np.mean(score))\n",
    "BO_stk_lgbm = BayesianOptimization(lgbm_stk_opt, pbounds, random_state=0)\n",
    "BO_stk_lgbm.maximize(init_points=50, n_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52cb9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "BO_stk_lgbm.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473c6451",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_params_rid = BO_stk_rid.max['params']\n",
    "max_params_las = BO_stk_las.max['params']\n",
    "max_params_ela = BO_stk_ela.max['params']\n",
    "max_params_ard = BO_stk_ard.max['params']\n",
    "max_params_bay = BO_stk_bay.max['params']\n",
    "max_params_lgbm = BO_stk_lgbm.max['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd267d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_params_ard['n_iter'] = int(round(max_params_ard['n_iter']))\n",
    "\n",
    "max_params_bay['n_iter'] = int(round(max_params_bay['n_iter']))\n",
    "\n",
    "max_params_lgbm['num_leaves'] = int(round(max_params_lgbm['num_leaves']))\n",
    "max_params_lgbm['n_estimators'] = int(round(max_params_lgbm['n_estimators']))\n",
    "max_params_lgbm['max_depth'] = int(round(max_params_lgbm['max_depth']))\n",
    "max_params_lgbm['min_child_samples'] = int(round(max_params_lgbm['min_child_samples']))\n",
    "max_params_lgbm['min_child_weight'] = int(round(max_params_lgbm['min_child_weight']))\n",
    "max_params_lgbm['max_bin'] = int(round(max_params_lgbm['max_bin']))\n",
    "max_params_lgbm['subsample'] = max(min(max_params_lgbm['subsample'], 1), 0)\n",
    "max_params_lgbm['colsample_bytree'] = max(min(max_params_lgbm['colsample_bytree'], 1), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b092593",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_params_rid,'\\n',max_params_las,'\\n',max_params_ela,'\\n',max_params_ard,'\\n',max_params_bay,'\\n',max_params_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc01e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "stks_tuned = [Ridge(random_state=0, **max_params_rid),Lasso(random_state=0, **max_params_las),ElasticNet(random_state=0, **max_params_ela),\n",
    "             ARDRegression(**max_params_ard),BayesianRidge(**max_params_bay),LGBMRegressor(random_state=0,**max_params_lgbm),CatBoostRegressor(random_state=0)]\n",
    "stks_tuned = [(str(stk).split('(')[0], stk) for stk in stks_tuned]\n",
    "stks_tuned[-1] = list(stks_tuned[-1])\n",
    "stks_tuned[-1][0] = 'CatBoostRegressor'\n",
    "stks_tuned[-1] = tuple(stks_tuned[-1])\n",
    "\n",
    "stks_trained = [(name, stk.fit(S_train2, y_train2),mean_squared_error(stk.predict(S_dev),y_dev,squared=False))\n",
    "                    for name, stk in tqdm(stks_tuned.copy())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d51c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stks_tuned = [Ridge(random_state=0, **max_params_rid),Lasso(random_state=0, **max_params_las),ElasticNet(random_state=0, **max_params_ela),\n",
    "             ARDRegression(**max_params_ard),BayesianRidge(**max_params_bay),LGBMRegressor(random_state=0,**max_params_lgbm),CatBoostRegressor(random_state=0)]\n",
    "stks_tuned = [(str(stk).split('(')[0], stk) for stk in stks_tuned]\n",
    "stks_tuned[-1] = list(stks_tuned[-1])\n",
    "stks_tuned[-1][0] = 'CatBoostRegressor'\n",
    "stks_tuned[-1] = tuple(stks_tuned[-1])\n",
    "\n",
    "stks_trained_for_submissions = [(name, stk.fit(S_train,y_train)) for name, stk in tqdm(stks_tuned.copy())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9a8d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stks_tuned = [Ridge(random_state=0, **max_params_rid),Lasso(random_state=0, **max_params_las),ElasticNet(random_state=0, **max_params_ela),\n",
    "             ARDRegression(**max_params_ard),BayesianRidge(**max_params_bay),LGBMRegressor(random_state=0,**max_params_lgbm),CatBoostRegressor(random_state=0)]\n",
    "stks_tuned = [(str(stk).split('(')[0], stk) for stk in stks_tuned]\n",
    "stks_tuned[-1] = list(stks_tuned[-1])\n",
    "stks_tuned[-1][0] = 'CatBoostRegressor'\n",
    "stks_tuned[-1] = tuple(stks_tuned[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193961ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results = []\n",
    "for name, stk, stk_score in stks_trained:\n",
    "    pred = list(stk.predict(S_dev))\n",
    "    name = f'{name} \\n({stk_score:.4f})'\n",
    "    pred_results.append(pd.Series(pred, name=name))\n",
    "ensemble_results = pd.concat(pred_results, axis=1)\n",
    "ensemble_results = ensemble_results.applymap(lambda x: float(x))\n",
    "\n",
    "# 모형의 예측값 간의 상관관계를 보기 위해 hitmap을 도식한다.\n",
    "plt.figure(figsize = (8,6))\n",
    "g = sns.heatmap(ensemble_results.corr(), annot=True, cmap='Blues')\n",
    "g.set_title(\"Correlation between models\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511f3b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = (ensemble_results.corr().sum()-1)/(ensemble_results.corr().shape[0]-1)\n",
    "names = corr.index\n",
    "rmse = np.array(corr.index.str[-7:-1]).astype(float)\n",
    "df = pd.DataFrame({'model': names, 'rmse': rmse, 'cor': corr})        \n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "g = sns.scatterplot(x=\"cor\", y=\"rmse\", data=df, s=40, color='red')\n",
    "for line in range(0, df.shape[0]):\n",
    "     g.text(df.cor[line]+0.003, df.rmse[line]-0.003, \n",
    "            df.model[line], horizontalalignment='left', \n",
    "            size='medium', color='black', weight='semibold')\n",
    "        \n",
    "plt.xlim((df.cor.min()-0.01,df.cor.max()+0.01))\n",
    "plt.ylim((df.rmse.min()-0.01,df.rmse.max()+0.01))\n",
    "plt.xlabel('Mean Agreement')\n",
    "plt.ylabel('RMSE')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2593239b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selected = [#'LinearRegression',\n",
    "            #'Ridge',\n",
    "            'Lasso',\n",
    "            #'ElasticNet',\n",
    "            #'ARDRegression',\n",
    "            #'BayesianRidge',\n",
    "            #'RandomForestRegressor',\n",
    "            #'XGBRegressor',\n",
    "            'LGBMRegressor',\n",
    "            #'CatBoostRegressor'\n",
    "            ]\n",
    "models_for_ensemble = [(name,stk) for name,stk,score in stks_trained if name in selected]\n",
    "\n",
    "weights_stk = []\n",
    "rmse_best = 1000\n",
    "for i in tqdm(range(0, 21, 1)):\n",
    "    for j in range(0, 21, 1):\n",
    "        if (i+j) != 20:\n",
    "            continue\n",
    "        pred = (models_for_ensemble[0][1].predict(S_dev) * i + models_for_ensemble[1][1].predict(S_dev) * j)/20\n",
    "        rmse = np.sqrt(mean_squared_error(y_dev, pred))\n",
    "        if rmse < rmse_best:\n",
    "            weights_stk = [i,j]\n",
    "            rmse_best = rmse \n",
    "            print(rmse, i,j)            \n",
    "\n",
    "print(rmse_best, weights_stk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78f663f",
   "metadata": {},
   "outputs": [],
   "source": [
    "w0,w1 = weights_stk\n",
    "selected = [#'LinearRegression',\n",
    "            #'Ridge',\n",
    "            'Lasso',\n",
    "            #'ElasticNet',\n",
    "            #'ARDRegression',\n",
    "            #'BayesianRidge',\n",
    "            #'RandomForestRegressor',\n",
    "            #'XGBRegressor',\n",
    "            'LGBMRegressor',\n",
    "            #'CatBoostRegressor'\n",
    "            ]\n",
    "models_for_ensemble = [(name,stk) for name,stk,score in stks_trained if name in selected]\n",
    "stk_ensemble_val = (models_for_ensemble[0][1].predict(S_dev) * w0 + models_for_ensemble[1][1].predict(S_dev) * w1)/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a1c8ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weights_ds = []\n",
    "rmse_best = 1000\n",
    "for i in tqdm(range(0, 21, 1)):\n",
    "    for j in range(0, 21, 1):\n",
    "        if (i+j) != 20:\n",
    "            continue\n",
    "        pred = (stk_ensemble_val*i+dnn.predict(x_dev_cat).flatten()*j)/20\n",
    "        rmse = np.sqrt(mean_squared_error(y_dev, pred))\n",
    "        if rmse < rmse_best:\n",
    "            weights_ds = [i,j]\n",
    "            rmse_best = rmse \n",
    "            print(rmse, i,j)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8038f984",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(pd.DataFrame(dict(zip(['dnn','avg','stk'],[dnn.predict(x_dev_cat).flatten(),avg_test_result,stk_ensemble_val]))).corr(), annot=True, cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7967106d",
   "metadata": {},
   "source": [
    "### Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dad9021",
   "metadata": {},
   "outputs": [],
   "source": [
    "w0,w1 = weights_stk\n",
    "selected = [#'LinearRegression',\n",
    "            #'Ridge',\n",
    "            'Lasso',\n",
    "            #'ElasticNet',\n",
    "            #'ARDRegression',\n",
    "            #'BayesianRidge',\n",
    "            #'RandomForestRegressor',\n",
    "            #'XGBRegressor',\n",
    "            'LGBMRegressor',\n",
    "            #'CatBoostRegressor'\n",
    "            ]\n",
    "models_for_ensemble = [(name,stk) for name,stk in stks_trained_for_submissions if name in selected]\n",
    "stk_ensemble_results = (models_for_ensemble[0][1].predict(S_test) * w0 + models_for_ensemble[1][1].predict(S_test) * w1)/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28481575",
   "metadata": {},
   "outputs": [],
   "source": [
    "w0, w1 = weights_ds\n",
    "pd.DataFrame({'age': (stk_ensemble_val*w0+dnn.predict(x_dev_cat).flatten()*w1)/20}).to_csv('categorical_stken14laslgbm_rid_lgbm_cat_dnn6_dev.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174ff89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "w0, w1 = weights_ds\n",
    "pd.DataFrame({'custid': test_id, 'age': (stk_ensemble_results*w0+dnn.predict(x_test_cat).flatten()*w1)/20}).to_csv('categorical_stken14laslgbm_rid_lgbm_cat_dnn6.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
