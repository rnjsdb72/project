{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30878bc6",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5b14b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "plt.rc('font',family='malgun gothic')\n",
    "plt.rc('axes',unicode_minus=False)\n",
    "import seaborn as sns\n",
    "\n",
    "# EDA\n",
    "import klib\n",
    "\n",
    "# Preprocessing&Feature Engineering\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PowerTransformer, RobustScaler, MaxAbsScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from bayes_opt import BayesianOptimization\n",
    "import kerastuner as kt\n",
    "\n",
    "# Modeling\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, ARDRegression, BayesianRidge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import VotingRegressor, StackingRegressor\n",
    "import tensorflow as tf\n",
    "from vecstack import StackingTransformer\n",
    "from vecstack import stacking\n",
    "\n",
    "# Eveluation\n",
    "from sklearn.metrics import mean_squared_error # squared=False시 RMSE\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Utility\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "from IPython.display import Image\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import platform\n",
    "from itertools import combinations\n",
    "from scipy.stats.mstats import gmean\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fa4917",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb6b5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train =  pd.read_csv(os.path.abspath(\"../input\")+'/X_train.csv', encoding='cp949')\n",
    "y_train =  pd.read_csv(os.path.abspath(\"../input\")+'/y_train.csv', encoding='cp949')\n",
    "df_test =  pd.read_csv(os.path.abspath(\"../input\")+'/X_test.csv', encoding='cp949')\n",
    "test_id = df_test['custid'].unique()\n",
    "y_train=y_train.set_index('custid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0f41cb",
   "metadata": {},
   "source": [
    "### Feature Generation&Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3619426",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tr = pd.concat([df_train, df_test])\n",
    "tr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d465a3",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc226f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 차원축소 매소드 \n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def dummy_to_pca(tr, column_name:str) :\n",
    "    max_seq = 300\n",
    "    max_d = 15\n",
    "    col_count = tr.groupby(column_name)[column_name].count()\n",
    "    if len(col_count) > max_seq:\n",
    "        tops = col_count.sort_values(ascending=False)[0:max_seq].index\n",
    "        f =tr.loc[tr[column_name].isin(tops)][['custid', column_name]]\n",
    "    else:\n",
    "        tops = col_count.index\n",
    "        f =tr[['custid', column_name]]\n",
    "    f = pd.get_dummies(f, columns=[column_name])  # This method performs One-hot-encoding\n",
    "    f = f.groupby('custid').mean()\n",
    "    if len(tops) < max_d:\n",
    "        max_d = len(tops)\n",
    "    pca = PCA(n_components=max_d)\n",
    "    pca.fit(f)\n",
    "    cumsum = np.cumsum(pca.explained_variance_ratio_) #분산의 설명량을 누적합\n",
    "    print(cumsum)\n",
    "    num_d = np.argmax(cumsum >= 0.99) + 1 # 분산의 설명량이 99%이상 되는 차원의 수\n",
    "    if num_d == 1:\n",
    "        num_d = max_d\n",
    "    pca = PCA(n_components=num_d)    \n",
    "    result = pca.fit_transform(f)\n",
    "    result = pd.DataFrame(result)\n",
    "    result.columns = [column_name + '_' + str(column) for column in result.columns]\n",
    "    result.index = f.index\n",
    "    return result.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f68769",
   "metadata": {},
   "outputs": [],
   "source": [
    "features=pd.DataFrame({'custid':tr.custid.unique()})\n",
    "f = dummy_to_pca(tr, 'brd_nm'); features = pd.merge(features,f,on='custid',how='left')\n",
    "f = dummy_to_pca(tr, 'corner_nm'); features = pd.merge(features,f,on='custid',how='left')\n",
    "f = dummy_to_pca(tr, 'pc_nm'); features = pd.merge(features,f,on='custid',how='left')\n",
    "f = dummy_to_pca(tr, 'part_nm'); features = pd.merge(features,f,on='custid',how='left')\n",
    "f = dummy_to_pca(tr, 'buyer_nm'); features = pd.merge(features,f,on='custid',how='left')\n",
    "f = dummy_to_pca(tr, 'team_nm'); features = pd.merge(features,f,on='custid',how='left')\n",
    "f = dummy_to_pca(tr, 'goodcd'); features = pd.merge(features,f,on='custid',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e475d0",
   "metadata": {},
   "source": [
    "### W2V-상품분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e63a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_vec():\n",
    "    sentences = []\n",
    "    df_all = df_train\n",
    "    for id in tqdm(df_all.custid.unique()):\n",
    "        x = df_all.query('custid == @id')[level].unique()\n",
    "        y = y_train.query('custid == @id').age\n",
    "        for j in range(20):\n",
    "            y = np.append(y, np.random.choice(x, len(x), replace=False))\n",
    "        sentences.append(list(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e292eca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "level = 'corner_nm' # 상품 분류 수준\n",
    "\n",
    "# W2V 학습을 하기에는 데이터(즉 corpus)가 부족하여 \n",
    "# 고객별로 구매한 상품 목록으로부터 20배 oversampling을 수행\n",
    "sentences = []\n",
    "df_all = pd.concat([df_train, df_test])\n",
    "for id in tqdm(df_all.custid.unique()):\n",
    "    uw = df_all.query('custid == @id')[level].unique()\n",
    "    bs = np.array([])\n",
    "    for j in range(20):\n",
    "        bs = np.append(bs, np.random.choice(uw, len(uw), replace=False))\n",
    "    sentences.append(list(bs))\n",
    "    sentences.append(list(df_all.query('custid == @id')[level].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d760d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 300 # 문자 벡터 차원 수\n",
    "min_word_count = 1 # 최소 문자 수\n",
    "num_workers = 4 # 병렬 처리 스레드 수\n",
    "context = 3 # 문자열 창 크기\n",
    "downsampling = 1e-3 # 문자 빈도수 Downsample\n",
    "\n",
    "# 모델 학습\n",
    "model = word2vec.Word2Vec(sentences, \n",
    "                          workers=num_workers, \n",
    "#                           size=max_features, \n",
    "                          min_count=min_word_count,\n",
    "                          window=context,\n",
    "                          sample=downsampling)\n",
    "# 필요없는 메모리 unload\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d396fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make features based on Word2Vec\n",
    "# 고객별로 구매한 상품의 평균벡터를 feature로 사용한다.\n",
    "features_wv = []\n",
    "for id in tqdm(df_train.custid.unique()):\n",
    "    features_wv.append(df_all.query('custid == @id')[level] \\\n",
    "                              .apply(lambda x: model.wv[x]).mean())\n",
    "X_train2 = np.array(features_wv)\n",
    "\n",
    "features_wv = []\n",
    "for id in tqdm(df_test.custid.unique()):\n",
    "    features_wv.append(df_all.query('custid == @id')[level] \\\n",
    "                              .apply(lambda x: model.wv[x]).mean())\n",
    "X_test2 = np.array(features_wv)\n",
    "\n",
    "X_train2 = pd.DataFrame(X_train2)\n",
    "X_test2 = pd.DataFrame(X_test2)\n",
    "X_train2.insert(0,'custid',df_train.custid.unique())\n",
    "X_test2.insert(0,'custid',df_test.custid.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16252920",
   "metadata": {},
   "source": [
    "### W2V-브랜드명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c347d263",
   "metadata": {},
   "outputs": [],
   "source": [
    "### W2V-브랜드명level = 'brd_nm' # 상품 분류 수준\n",
    "\n",
    "# W2V 학습을 하기에는 데이터(즉 corpus)가 부족하여 \n",
    "# 고객별로 구매한 상품 목록으로부터 20배 oversampling을 수행\n",
    "sentences = []\n",
    "df_all = pd.concat([df_train, df_test])\n",
    "for id in tqdm(df_all.custid.unique()):\n",
    "    uw = df_all.query('custid == @id')[level].unique()\n",
    "    bs = np.array([])\n",
    "    for j in range(20):\n",
    "        bs = np.append(bs, np.random.choice(uw, len(uw), replace=False))\n",
    "    sentences.append(list(bs))\n",
    "    sentences.append(list(df_all.query('custid == @id')[level].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50228d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 300 # 문자 벡터 차원 수\n",
    "min_word_count = 1 # 최소 문자 수\n",
    "num_workers = 4 # 병렬 처리 스레드 수\n",
    "context = 3 # 문자열 창 크기\n",
    "downsampling = 1e-3 # 문자 빈도수 Downsample\n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# 모델 학습\n",
    "model = word2vec.Word2Vec(sentences, \n",
    "                          workers=num_workers, \n",
    "#                           size=max_features, \n",
    "                          min_count=min_word_count,\n",
    "                          window=context,\n",
    "                          sample=downsampling)\n",
    "# 필요없는 메모리 unload\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff36d1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make features based on Word2Vec\n",
    "# 고객별로 구매한 상품의 평균벡터를 feature로 사용한다.\n",
    "features_wv = []\n",
    "for id in tqdm(df_train.custid.unique()):\n",
    "    features_wv.append(df_all.query('custid == @id')[level] \\\n",
    "                              .apply(lambda x: model.wv[x]).mean())\n",
    "X_train3 = np.array(features_wv)\n",
    "\n",
    "features_wv = []\n",
    "for id in tqdm(df_test.custid.unique()):\n",
    "    features_wv.append(df_all.query('custid == @id')[level] \\\n",
    "                              .apply(lambda x: model.wv[x]).mean())\n",
    "X_test3 = np.array(features_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337f2862",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train3 = pd.DataFrame(X_train3)\n",
    "X_test3 = pd.DataFrame(X_test3)\n",
    "X_train3.insert(0,'custid',df_train.custid.unique())\n",
    "X_test3.insert(0,'custid',df_test.custid.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdb3a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 = pd.read_excel('X_train2.xlsx',index_col=0)\n",
    "X_test2 = pd.read_excel('X_test2.xlsx',index_col=0)\n",
    "X_train3 = pd.read_excel('X_train3.xlsx',index_col=0)\n",
    "X_test3 = pd.read_excel('X_test3.xlsx',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2191f4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_modify(x):\n",
    "    if x > 12:\n",
    "        return x-12\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def extract_hour(x):\n",
    "    if len(str(x))>3:\n",
    "        return str(x)[:2]\n",
    "    else: \n",
    "        return str(x)[:1]\n",
    "def extract_season(x):\n",
    "    if 3 <= x <= 5 :\n",
    "        return('봄')\n",
    "    elif 6 <= x <= 8 :\n",
    "        return('여름')\n",
    "    elif 9 <= x <= 11 :    \n",
    "        return('가을')\n",
    "    else :\n",
    "        return('겨울') \n",
    "def time_(x):\n",
    "    if 9 <= x <= 11 :\n",
    "        return('아침_구매건수')\n",
    "    elif 12 <= x <= 17 :\n",
    "        return('점심_구매건수')\n",
    "    else :\n",
    "        return('저녁_구매건수')\n",
    "def half_year(x):\n",
    "    if 1<=x<=5:\n",
    "        return('전반기')\n",
    "    else:\n",
    "        return('후반기')\n",
    "def peak_season(x):\n",
    "    if x in [7,8,12,1,2]:\n",
    "        return('성수기')\n",
    "    else:\n",
    "        return('비성수기')\n",
    "def div_month(x):\n",
    "    if 1<=x<=10:\n",
    "        return('월초')\n",
    "    elif 11<=x<=20:\n",
    "        return('월중')\n",
    "    else:\n",
    "        return('월말')\n",
    "def noon(x):\n",
    "    if int(str(x)[-2:])<=12:\n",
    "        return('오전')\n",
    "    else:\n",
    "        return('오후')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76001c8d",
   "metadata": {},
   "source": [
    "일반변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cea45b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tr.copy()\n",
    "df['sales_hour'] = df.sales_time.apply(extract_hour)\n",
    "df['sales_hour'] = pd.to_numeric(df['sales_hour'])\n",
    "df['sales_hour'] = np.where(df['sales_hour'] < 9, 21, df['sales_hour'])\n",
    "df['방문시간대'] = df.sales_hour.apply(time_)\n",
    "df['sales_month'] = df.sales_month.apply(month_modify)\n",
    "df['반기'] = df.sales_month.apply(half_year)\n",
    "df['성수기여부'] = df.sales_month.apply(peak_season)\n",
    "df['월_초중말'] = df.sales_day.apply(div_month)\n",
    "df['오전/오후'] = df.sales_time.apply(noon)\n",
    "\n",
    "# 총구매액\n",
    "f = df.groupby('custid')['tot_amt'].agg([('총구매액', 'sum')]).reset_index()\n",
    "features = pd.merge(features,f, on = 'custid')\n",
    "# 평균구매액\n",
    "f = df.groupby('custid')['tot_amt'].agg([('평균구매액', 'mean')]).reset_index()\n",
    "features = pd.merge(features,f, on = 'custid')\n",
    "\n",
    "# 총할인금액\n",
    "f = pd.DataFrame(df.groupby('custid').dis_amt.sum()).rename(columns={'dis_amt':'총할인금액'})\n",
    "features = pd.merge(features,f, on = 'custid')\n",
    "# 평균할인금액\n",
    "f = pd.DataFrame(df.groupby('custid').dis_amt.mean()).rename(columns={'dis_amt':'평균할인금액'})\n",
    "features = pd.merge(features,f, on = 'custid')\n",
    "\n",
    "# 총실구매액\n",
    "f = pd.DataFrame(df.groupby('custid').net_amt.sum()).rename(columns={'net_amt':'총실구매액'})\n",
    "features = pd.merge(features,f, on = 'custid')\n",
    "# 평균실구매액\n",
    "f = pd.DataFrame(df.groupby('custid').net_amt.mean()).rename(columns={'net_amt':'평균실구매액'})\n",
    "features = pd.merge(features,f, on = 'custid')\n",
    "\n",
    "# 요일별 방문횟수\n",
    "f = pd.crosstab(df.custid,df.sales_dayofweek, margins=True).reindex(columns=['월','화','수',\n",
    "                                                        '목','금','토','일','All']).iloc[:-1,:].rename(columns={'All':'총방문횟수'})\n",
    "features = pd.merge(features,f, on = 'custid')\n",
    "\n",
    "# 시간별 방문횟수\n",
    "f = pd.crosstab(df.custid, df.sales_hour).rename(columns=dict(zip(df.sales_hour.unique(),[str(i)+'시방문' for i in df.sales_hour.unique()])))\n",
    "features = pd.merge(features,f, on = 'custid')\n",
    "\n",
    "# 수입상품 구매 건수\n",
    "f = pd.DataFrame(df.groupby('custid').import_flg.sum()).rename(columns={'import_flg':'수입상품구매건수'})\n",
    "features = pd.merge(features,f, on = 'custid')\n",
    "\n",
    "# x = df[df['import_flg'] == 1].groupby('custid').size()\n",
    "# f = x.reset_index().rename(columns={0: '수입상품_구매건수'}).fillna(0)\n",
    "# features = pd.merge(features,f, on = 'custid')\n",
    "\n",
    "# 구매건수\n",
    "f = df.groupby('custid')['tot_amt'].agg([('구매건수', 'size')]).reset_index()\n",
    "features = pd.merge(features,f, on = 'custid')\n",
    "\n",
    "# 평균할부개월수\n",
    "f = df.groupby('custid')['inst_mon'].agg([('평균할부개월수', 'mean')]).reset_index()\n",
    "features = pd.merge(features,f, on = 'custid')\n",
    "\n",
    "# 지점별 방문횟수\n",
    "f = pd.crosstab(df.custid, df.str_nm).rename(columns=dict(zip(df.str_nm.unique(),[i+'방문' for i in df.str_nm.unique()])))\n",
    "features = pd.merge(features,f, on = 'custid')\n",
    "\n",
    "# 방문지점 수\n",
    "f = df.groupby('custid')['str_nm'].agg([('방문지점수',lambda x: x.nunique())])\n",
    "features = pd.merge(features,f, on = 'custid')\n",
    "\n",
    "# 월별 구매횟수\n",
    "f = pd.crosstab(df.custid,df.sales_month).rename(columns=dict(zip(df.sales_month.unique(), [str(i)+'월방문' for i in df.sales_month.unique()])))\n",
    "features = pd.merge(features,f, on = 'custid')\n",
    "\n",
    "# 구매일수\n",
    "df['sales_month'] = df['sales_month'].astype(str)\n",
    "df['sales_day'] = df['sales_day'].astype(str)\n",
    "df['판매일'] = df['sales_month'] + '-' + df['sales_day']\n",
    "df.판매일 = pd.to_datetime(df.판매일,format='%m-%d')\n",
    "f = df.groupby(by = 'custid')['판매일'].agg([('구매일수','nunique')]).reset_index()\n",
    "features = pd.merge(features,f, on = 'custid')\n",
    "\n",
    "# 구매주기\n",
    "f = df.groupby('custid')['판매일'].agg([('구매주기', lambda x: int((x.astype('datetime64').max() - x.astype('datetime64').min()).days / x.nunique()))]).reset_index()\n",
    "features = pd.merge(features,f, on = 'custid')\n",
    "\n",
    "# 최고 구입 금액\n",
    "f = df.groupby('custid')['tot_amt'].agg([('최고구매금액', 'max')]).reset_index()\n",
    "features = pd.merge(features,f, on = 'custid')\n",
    "\n",
    "# 계절별 구매건수\n",
    "df['sales_month'] = pd.to_numeric(df['sales_month'])\n",
    "df['계절'] = df.sales_month.apply(extract_season)\n",
    "f = pd.pivot_table(df, index = 'custid', columns = '계절', values = 'tot_amt',\n",
    "                  aggfunc = np.size, fill_value = 0).reset_index()\n",
    "features = pd.merge(features,f, on = 'custid')\n",
    "\n",
    "# 반기별 구매건수\n",
    "f = pd.crosstab(df.custid, df.반기).reset_index()\n",
    "features = pd.merge(features,f, on = 'custid')\n",
    "\n",
    "# 성수기 여부 별 구매건수\n",
    "f = pd.crosstab(df.custid, df.성수기여부).reset_index()\n",
    "features = pd.merge(features,f, on = 'custid')\n",
    "\n",
    "# 월 시기별 구매건수\n",
    "f = pd.crosstab(df.custid, df.월_초중말).reset_index()\n",
    "features = pd.merge(features,f, on = 'custid')\n",
    "\n",
    "# 오전/오후별 구매건수\n",
    "f = pd.crosstab(df.custid, df['오전/오후']).reset_index()\n",
    "features = pd.merge(features,f, on = 'custid')\n",
    "\n",
    "# 방문지점 갯수\n",
    "f = df.groupby(by = 'custid')['str_nm'].agg([('방문지점개수','nunique')]).reset_index()\n",
    "features = pd.merge(features,f, on = 'custid')\n",
    "\n",
    "# 구매상품 다양성\n",
    "n = df.corner_nm.nunique()\n",
    "f = df.groupby('custid')['goodcd'].agg([('구매상품다양성', lambda x: len(x.unique()) / n)]).reset_index()\n",
    "features = pd.merge(features,f, on = 'custid')\n",
    "\n",
    "# 시간대별 방문횟수\n",
    "f = pd.crosstab(df.custid, df.방문시간대)\n",
    "features = pd.merge(features,f, on = 'custid')\n",
    "\n",
    "# 주구매코너\n",
    "#f = df.groupby('custid')['corner_nm'].agg([('주구매코너', lambda x: x.value_counts().index[0])]).reset_index()\n",
    "#f = pd.get_dummies(f, columns=['주구매코너'])  # This method performs One-hot-encoding\n",
    "#features = pd.merge(features,f, on = 'custid')\n",
    "\n",
    "# 코너별 구매건수\n",
    "f = pd.pivot_table(df, index='custid', columns='part_nm', values='tot_amt', \n",
    "                   aggfunc=np.size, fill_value=0)\n",
    "f = f.rename(columns=dict(zip(f.columns,[i+'_구매건수' for i in f.columns]))).reset_index()\n",
    "features = pd.merge(features,f, on = 'custid')\n",
    "\n",
    "# 바이어 이름 별 구매건수\n",
    "f = pd.pivot_table(df, index='custid', columns='buyer_nm', values='tot_amt', \n",
    "                   aggfunc=np.size, fill_value=0)\n",
    "f = f.rename(columns=dict(zip(f.columns,[i+'_구매건수' for i in f.columns]))).reset_index()\n",
    "features = pd.merge(features,f, on = 'custid')\n",
    "\n",
    "# 무이자 할부 평균 가격\n",
    "f = df.loc[df.inst_fee==1].groupby('custid').net_amt.agg([('무이자할부평균가격','mean')]).reset_index()\n",
    "features = pd.merge(features,f,on='custid',how='left').fillna(0)\n",
    "\n",
    "# 할부결제건수\n",
    "f = df.loc[df.inst_mon>1].groupby('custid').inst_mon.agg([('할부결제건수','count')])\n",
    "features = pd.merge(features,f,on='custid',how='left').fillna(0)\n",
    "\n",
    "# 내점 당 구매금액\n",
    "f = df.groupby(['custid','판매일'])['tot_amt'].sum().reset_index().groupby('custid')['tot_amt'].agg([('내점당구매금액','mean')]).reset_index()\n",
    "features = pd.merge(features,f,on='custid',how='left')\n",
    "\n",
    "# 내점 당 구매개수\n",
    "f = df.groupby(['custid','판매일'])['tot_amt'].count().reset_index().groupby('custid')['tot_amt'].agg([('내점당구매개수','mean')]).reset_index()\n",
    "features = pd.merge(features,f,on='custid',how='left')\n",
    "\n",
    "# 평균쇼핑시간\n",
    "f = df.groupby(['custid','판매일'])['sales_time'].agg(lambda x: x.max()-x.min()).reset_index().groupby('custid').sales_time.agg([('평균쇼핑시간','mean')])\n",
    "features = pd.merge(features,f,on='custid',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d6f090",
   "metadata": {},
   "source": [
    "```python\n",
    "# 파트별 구매액 합\n",
    "f = pd.pivot_table(tr, index='custid', columns='part_nm', values='tot_amt', \n",
    "                   aggfunc='sum', fill_value=0).reset_index()\n",
    "features = pd.merge(features,f, on = 'custid')\n",
    "\n",
    "# 파트별 할인금액\n",
    "f = pd.pivot_table(df, index='custid', columns='part_nm', values='dis_amt', \n",
    "                   aggfunc='sum', fill_value=0)\n",
    "f = f.rename(columns=dict(zip(f.columns,[i+'_구매건수' for i in f.columns]))).reset_index()\n",
    "features = pd.merge(features,f, on = 'custid')\n",
    "\n",
    "# part_nm별 구매건수\n",
    "f = pd.pivot_table(df, index='custid', columns='part_nm', values='tot_amt', \n",
    "                   aggfunc=np.size, fill_value=0)\n",
    "f = f.rename(columns=dict(zip(f.columns,[i+'_구매건수' for i in f.columns]))).reset_index()\n",
    "features = pd.merge(features,f, on = 'custid')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45010340",
   "metadata": {},
   "outputs": [],
   "source": [
    "del features['custid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cab35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이상치 처리\n",
    "features = features.apply(lambda x: x.clip(x.quantile(0.05), x.quantile(0.95)), axis=0)\n",
    "\n",
    "# 표준화\n",
    "features.loc[:,:] = RobustScaler().fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15650f24",
   "metadata": {},
   "source": [
    "비율변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c1f619",
   "metadata": {},
   "outputs": [],
   "source": [
    "features['custid'] = tr.custid.unique()\n",
    "\n",
    "tr['sales_hour'] = tr.sales_time.apply(extract_hour)\n",
    "weekdays = pd.crosstab(tr.custid,tr.sales_dayofweek, margins=True).reindex(columns=['월','화','수',\n",
    "                                                        '목','금','토','일','All']).iloc[:-1,:].rename(columns={'All':'총방문횟수'})\n",
    "sales_hour = pd.crosstab(df.custid, df.sales_hour, margins=True)\n",
    "sales_hour = sales_hour.rename(columns=dict(zip(sales_hour.columns,[str(i)+'시방문' for i in sales_hour.columns])))\n",
    "str_nm = pd.crosstab(tr.custid, tr.str_nm,margins=True)\n",
    "str_nm = str_nm.rename(columns=dict(zip(str_nm.columns,[i+'방문' for i in str_nm.columns])))\n",
    "sales_month = pd.crosstab(df.custid,df.sales_month, margins=True)\n",
    "sales_month = sales_month.rename(columns=dict(zip(sales_month.columns, [str(i)+'월방문' for i in sales_month])))\n",
    "tr['sales_month'] = pd.to_numeric(tr['sales_month'])\n",
    "tr['sales_month'] = tr.sales_month.apply(month_modify)\n",
    "tr['season'] = pd.DataFrame(tr.sales_month.apply(extract_season))\n",
    "season_visit = pd.crosstab(tr.custid, tr.season)\n",
    "tr['mln'] = tr.sales_time.apply(lambda x: int(str(x)[:2])).apply(time_)\n",
    "mln = pd.crosstab(tr.custid, tr.mln,margins=True)\n",
    "inv = tr.loc[tr.inst_mon>1].groupby('custid').inst_mon.agg([('할부결제건수','count')])\n",
    "trans_amount = tr.groupby('custid')['tot_amt'].agg([('구매건수', 'size')])\n",
    "peak = pd.crosstab(df.custid, df.성수기여부, margins=True)\n",
    "peak = peak.divide(peak.iloc[:,-1],axis=0).iloc[:-1,:-1]\n",
    "half = pd.crosstab(df.custid, df.반기, margins=True)\n",
    "half = half.divide(half.iloc[:,-1],axis=0).iloc[:-1,:-1]\n",
    "noon = pd.crosstab(df.custid, df['오전/오후'], margins=True)\n",
    "noon = noon.divide(noon.iloc[:,-1],axis=0).iloc[:-1,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba00e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수입상품 구매비율\n",
    "x = df[df['import_flg'] == 1].groupby('custid').size() / df.groupby('custid').size()\n",
    "f = x.reset_index().rename(columns={0: '수입상품구매비율'}).fillna(0)\n",
    "f.iloc[:,1] = (f.iloc[:,1])\n",
    "features = pd.merge(features,f, on = 'custid')\n",
    "\n",
    "# 주말 방문 비율\n",
    "day_to_int = {\n",
    "    '월': 1,'화': 2,'수': 3,'목': 4,'금': 5,'토': 6,'일': 7}\n",
    "df2 = df.copy()\n",
    "df2['sales_dayofweek'] = df2['sales_dayofweek'].map(day_to_int)\n",
    "df2 = pd.pivot_table(df, index='custid', columns='sales_dayofweek', values='tot_amt', \n",
    "                   aggfunc=np.size, fill_value=0).reset_index();\n",
    "df2['주말방문비율'] = ((df2.iloc[:,5]+df2.iloc[:,6]) / (df2.iloc[:,1]+df2.iloc[:,2]+df2.iloc[:,3]+df2.iloc[:,4]+\n",
    "                                                df2.iloc[:,5]+df2.iloc[:,6]+df2.iloc[:,7]))\n",
    "f = df2[['custid','주말방문비율']]\n",
    "features = pd.merge(features,f, on = 'custid')\n",
    "\n",
    "# 요일별 방문비율\n",
    "f = weekdays.iloc[:,:-1].divide(weekdays.iloc[:,-1], axis=0).rename(columns=dict(zip(weekdays.columns,\n",
    "                                                                                    [str(i)+'_prop' for i in weekdays.columns])))\n",
    "features = pd.merge(features,f, on = 'custid')\n",
    "\n",
    "# 시간별 방문비율\n",
    "f = sales_hour.iloc[:-1,:-1].divide(sales_hour.iloc[:-1,-1],axis=0).rename(columns=dict(zip(sales_hour.columns,\n",
    "                                                                                    [str(i)+'_prop' for i in sales_hour.columns])))\n",
    "features = pd.merge(features,f, on = 'custid')\n",
    "# 지점별 방문비율\n",
    "f = str_nm.iloc[:-1,:-1].divide(str_nm.iloc[:-1,-1],axis=0).rename(columns=dict(zip(str_nm.columns,[str(i)+'_prop' for i in str_nm.columns]))).reset_index()\n",
    "features = pd.merge(features,f, on = 'custid')\n",
    "# 구매월별 방문비율\n",
    "f = sales_month.iloc[:-1,:-1].divide(sales_month.iloc[:-1,-1],axis=0).rename(columns=\n",
    "                                                        dict(zip(sales_month.columns,[str(i)+'_prop' for i in sales_month.columns]))).reset_index()\n",
    "features = pd.merge(features,f, on = 'custid')\n",
    "# 계절별 방문비율\n",
    "f = season_visit.divide(weekdays.총방문횟수,axis=0).rename(columns=dict(zip(season_visit.columns,[column+'_prop' for column in season_visit.columns]))).reset_index()\n",
    "features = pd.merge(features,f, on = 'custid')\n",
    "# 시간대별 방문비율\n",
    "f = mln.div(mln.iloc[:,-1], axis=0).iloc[:-1,:-1].reset_index().rename(columns=dict(zip(mln.columns,[i+'_prop' for i in mln.columns])))\n",
    "features = pd.merge(features,f, on = 'custid')\n",
    "# 할부결제비율\n",
    "f =(inv['할부결제건수']/trans_amount['구매건수']).reset_index().rename(columns={0:'할부결제비율'}).fillna(0)\n",
    "features = pd.merge(features,f, on = 'custid')\n",
    "# 성수기 방문비율\n",
    "f = peak.iloc[:,1].reset_index().rename(columns={'성수기':'성수기방문비율'})\n",
    "features = pd.merge(features,f, on = 'custid')\n",
    "# 비성수기 방문비율\n",
    "f = peak.iloc[:,0].reset_index().rename(columns={'비성수기':'비성수기방문비율'})\n",
    "features = pd.merge(features,f, on = 'custid')\n",
    "# 전반기 방문비율\n",
    "f = half.iloc[:,0].reset_index().rename(columns={'전반기':'전반기방문비율'})\n",
    "features = pd.merge(features,f, on = 'custid')\n",
    "# 후반기 방문비율\n",
    "f = half.iloc[:,1].reset_index().rename(columns={'후반기':'후반기방문비율'})\n",
    "features = pd.merge(features,f, on = 'custid')\n",
    "# 오전 방문비율\n",
    "f = noon.iloc[:,0].reset_index().rename(columns={'오전':'오전방문비율'})\n",
    "features = pd.merge(features,f, on = 'custid')\n",
    "# 오후 방문비율\n",
    "f = noon.iloc[:,1].reset_index().rename(columns={'오후':'오후방문비율'})\n",
    "features = pd.merge(features,f, on = 'custid')\n",
    "# 할인율 평균\n",
    "df['할인율'] = df.dis_amt/df.tot_amt\n",
    "f = df.groupby('custid')['할인율'].mean().reset_index()\n",
    "features = pd.merge(features,f,on='custid',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590291f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame({'custid': df_train.custid.unique()})\n",
    "X_train = pd.merge(X_train, features, how='left', on='custid')\n",
    "\n",
    "X_test = pd.DataFrame({'custid': df_test.custid.unique()})\n",
    "X_test = pd.merge(X_test, features, how='left', on='custid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f67d60",
   "metadata": {},
   "source": [
    "merge W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccc68ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.merge(X_train, X_train2, how='left', on='custid')\n",
    "X_train = X_train.set_index('custid')\n",
    "X_test = pd.merge(X_test, X_test2, how='left', on='custid')\n",
    "X_test = X_test.set_index('custid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1efe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.merge(X_train, X_train3, how='left', on='custid')\n",
    "X_train = X_train.set_index('custid')\n",
    "X_test = pd.merge(X_test, X_test3, how='left', on='custid')\n",
    "X_test = X_test.set_index('custid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9d934a",
   "metadata": {},
   "source": [
    "One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f23248",
   "metadata": {},
   "outputs": [],
   "source": [
    "level = 'corner_nm'\n",
    "df_train[level].nunique()\n",
    "IDtest = df_test.custid.unique()\n",
    "\n",
    "df_all = pd.concat([df_train, df_test])\n",
    "x_train = pd.pivot_table(df_all, index='custid', columns=level, values='tot_amt',\n",
    "                         aggfunc=lambda x: np.where(len(x) >=1, 1, 0), fill_value=0). \\\n",
    "                         reset_index(). \\\n",
    "                         query('custid not in @IDtest'). \\\n",
    "                         set_index('custid')\n",
    "x_test = pd.pivot_table(df_all, index='custid', columns=level, values='tot_amt',\n",
    "                         aggfunc=lambda x: np.where(len(x) >=1, 1, 0), fill_value=0). \\\n",
    "                         reset_index(). \\\n",
    "                         query('custid in @IDtest'). \\\n",
    "                         set_index('custid')\n",
    "\n",
    "x_train = pd.merge(x_train, X_train, on = 'custid')\n",
    "x_test = pd.merge(x_test, X_test, on = 'custid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f05840",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefe9474",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = x_train\n",
    "# 사용할 모델 설정 (속도가 빠른 모델 사용 권장)\n",
    "model = LinearRegression()\n",
    "\n",
    "# 각 특성과 타깃(class) 사이에 유의한 통계적 관계가 있는지 계산하여 특성을 선택하는 방법 \n",
    "# feature 개수 바꿔가며 성능 test한다.\n",
    "cv_scores = []\n",
    "for p in tqdm(range(5,100,1)):\n",
    "    X_new = SelectPercentile(percentile=p).fit_transform(x_train, y_train)    \n",
    "    cv_score = cross_val_score(model, X_new, y_train, scoring='neg_mean_squared_error', cv=5).mean()\n",
    "    cv_scores.append((p,cv_score))\n",
    "\n",
    "# Print the best percentile\n",
    "best_score = cv_scores[np.argmax([score for _, score in cv_scores])]\n",
    "print(best_score)\n",
    "\n",
    "# Plot the performance change with p\n",
    "plt.plot([k for k, _ in cv_scores], [score for _, score in cv_scores])\n",
    "plt.xlabel('Percent of features')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ce6036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 과적합을 피하기 위해 최적의 p값 주변의 값을 선택하는게 더 나은 결과를 얻을 수 있다. \n",
    "fs = SelectPercentile(percentile=best_score[0]).fit(x_train, y_train)\n",
    "x_train = fs.transform(x_train)\n",
    "x_test = fs.transform(x_test)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(features.columns[fs.get_support()].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4edd0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(x_train,columns=features.columns[fs.get_support()].tolist()).to_csv('x_train_1round.csv')\n",
    "pd.DataFrame(x_test,columns=features.columns[fs.get_support()].tolist()).to_csv('x_train_1round.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1977851",
   "metadata": {},
   "source": [
    "```python\n",
    "pd.DataFrame(x_train,columns=features.columns[fs.get_support()].tolist()).to_csv('x_train_1round.csv')\n",
    "pd.DataFrame(x_test,columns=features.columns[fs.get_support()].tolist()).to_csv('x_train_1round.csv')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abae0041",
   "metadata": {},
   "source": [
    "### Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34dfa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train2, x_dev, y_train2, y_dev = train_test_split(x_train, y_train, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d4e146",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pbounds = {\n",
    "    'alpha':(0,50)\n",
    "}\n",
    "def rid_opt(alpha):\n",
    "    params = {\n",
    "        'alpha':alpha\n",
    "    }\n",
    "    rid = Ridge(random_state=0, **params)\n",
    "    rid.fit(x_train2,y_train2)\n",
    "    score = mean_squared_error(rid.predict(x_dev),y_dev,squared=False)\n",
    "    return -score\n",
    "BO_rid = BayesianOptimization(rid_opt, pbounds, random_state=0)\n",
    "BO_rid.maximize(init_points=50, n_iter=50) # init_points: exploration, n_iter: iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a339c6b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pbounds = {\n",
    "    'alpha':(0,50)\n",
    "}\n",
    "def las_opt(alpha):\n",
    "    params = {\n",
    "        'alpha':alpha\n",
    "    }\n",
    "    las = Lasso(random_state=0, **params)\n",
    "    las.fit(x_train2,y_train2)\n",
    "    score = mean_squared_error(las.predict(x_dev),y_dev,squared=False)\n",
    "    return -score\n",
    "BO_las = BayesianOptimization(las_opt, pbounds, random_state=0)\n",
    "BO_las.maximize(init_points=50, n_iter=50) # init_points: exploration, n_iter: iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a65ca5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pbounds = {\n",
    "    'alpha':(0,50)\n",
    "}\n",
    "def ela_opt(alpha):\n",
    "    params = {\n",
    "        'alpha':alpha\n",
    "    }\n",
    "    ela = ElasticNet(random_state=0, **params)\n",
    "    ela.fit(x_train2,y_train2)\n",
    "    score = mean_squared_error(ela.predict(x_dev),y_dev,squared=False)\n",
    "    return -score\n",
    "BO_ela = BayesianOptimization(ela_opt, pbounds, random_state=0)\n",
    "BO_ela.maximize(init_points=50, n_iter=50) # init_points: exploration, n_iter: iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede47d9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pbounds = {\n",
    "    'n_iter':(100,1000),\n",
    "    'alpha_1':(0,50),\n",
    "    'alpha_2':(0,50),\n",
    "    'lambda_1':(0,10),\n",
    "    'lambda_2':(0,10)\n",
    "}\n",
    "def ard_opt(n_iter,alpha_1,alpha_2,lambda_1,lambda_2):\n",
    "    params = {\n",
    "        'n_iter':int(round(n_iter)),\n",
    "        'alpha_1':alpha_1,\n",
    "        'alpha_2':alpha_2,\n",
    "        'lambda_1':lambda_1,\n",
    "        'lambda_2':lambda_2\n",
    "    }\n",
    "    ard = ARDRegression(**params)\n",
    "    ard.fit(x_train2,y_train2)\n",
    "    score = mean_squared_error(ard.predict(x_dev),y_dev,squared=False)\n",
    "    return -score\n",
    "BO_ard = BayesianOptimization(ard_opt, pbounds, random_state=0)\n",
    "BO_ard.maximize(init_points=50, n_iter=50) # init_points: exploration, n_iter: iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4bad30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pbounds = {\n",
    "    'n_iter':(100,1000),\n",
    "    'alpha_1':(0,50),\n",
    "    'alpha_2':(0,50),\n",
    "    'lambda_1':(0,10),\n",
    "    'lambda_2':(0,10)\n",
    "}\n",
    "def bay_opt(n_iter,alpha_1,alpha_2,lambda_1,lambda_2):\n",
    "    params = {\n",
    "        'n_iter':int(round(n_iter)),\n",
    "        'alpha_1':alpha_1,\n",
    "        'alpha_2':alpha_2,\n",
    "        'lambda_1':lambda_1,\n",
    "        'lambda_2':lambda_2\n",
    "    }\n",
    "    bay = BayesianRidge(**params)\n",
    "    bay.fit(x_train2,y_train2)\n",
    "    score = mean_squared_error(bay.predict(x_dev),y_dev,squared=False)\n",
    "    return -score\n",
    "BO_bay = BayesianOptimization(bay_opt, pbounds, random_state=0)\n",
    "BO_bay.maximize(init_points=50, n_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ba7723",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pbounds = {\n",
    "    'n_estimators':(100,1000),\n",
    "    'learning_rate':(0,1),\n",
    "    'max_depth':(2, 32),\n",
    "    'num_leaves':(2, 64),\n",
    "    'min_child_samples':(10, 200),\n",
    "    'min_child_weight':(1, 50),\n",
    "    'subsample':(0.5, 1),\n",
    "    'colsample_bytree':(0.5, 1),\n",
    "    'max_bin':(10, 500),\n",
    "    'reg_lambda':(0.001, 10),\n",
    "    'reg_alpha':(0.01, 50)\n",
    "}\n",
    "def lgbm_opt(n_estimators, learning_rate, max_depth, num_leaves, min_child_samples, min_child_weight,\n",
    "             subsample, colsample_bytree, max_bin, reg_lambda, reg_alpha):\n",
    "    params = {\n",
    "        \"n_estimators\":int(round(n_estimators)), \n",
    "        \"learning_rate\":learning_rate,\n",
    "        'max_depth':int(round(max_depth)),\n",
    "        'num_leaves':int(round(num_leaves)),\n",
    "        'min_child_samples': int(round(min_child_samples)),\n",
    "        'min_child_weight': int(round(min_child_weight)),\n",
    "        'subsample':max(min(subsample, 1), 0),\n",
    "        'colsample_bytree':max(min(colsample_bytree, 1), 0),\n",
    "        'reg_lambda': reg_lambda,\n",
    "        'reg_alpha': reg_alpha\n",
    "    }\n",
    "    lgbm = LGBMRegressor(random_state=0, **params)\n",
    "    lgbm.fit(x_train2,y_train2)\n",
    "    score = mean_squared_error(lgbm.predict(x_dev),y_dev,squared=False)\n",
    "    return -score\n",
    "BO_lgbm = BayesianOptimization(lgbm_opt, pbounds, random_state=0)\n",
    "BO_lgbm.maximize(init_points=50, n_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6550df0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_params_rid = BO_rid.max['params']\n",
    "max_params_las = BO_las.max['params']\n",
    "max_params_ela = BO_ela.max['params']\n",
    "max_params_ard = BO_ard.max['params']\n",
    "max_params_bay = BO_bay.max['params']\n",
    "max_params_lgbm = BO_lgbm.max['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d058ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_params_ard['n_iter'] = int(round(max_params_ard['n_iter']))\n",
    "\n",
    "max_params_bay['n_iter'] = int(round(max_params_bay['n_iter']))\n",
    "\n",
    "max_params_lgbm['num_leaves'] = int(round(max_params_lgbm['num_leaves']))\n",
    "max_params_lgbm['n_estimators'] = int(round(max_params_lgbm['n_estimators']))\n",
    "max_params_lgbm['max_depth'] = int(round(max_params_lgbm['max_depth']))\n",
    "max_params_lgbm['min_child_samples'] = int(round(max_params_lgbm['min_child_samples']))\n",
    "max_params_lgbm['min_child_weight'] = int(round(max_params_lgbm['min_child_weight']))\n",
    "max_params_lgbm['max_bin'] = int(round(max_params_lgbm['max_bin']))\n",
    "max_params_lgbm['subsample'] = max(min(max_params_lgbm['subsample'], 1), 0)\n",
    "max_params_lgbm['colsample_bytree'] = max(min(max_params_lgbm['colsample_bytree'], 1), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbb94d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_params_rid,'\\n',max_params_las,'\\n',max_params_ela,'\\n',max_params_ard,'\\n',max_params_bay,'\\n',max_params_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d39fd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "regs_tuned = [Ridge(random_state=0, **max_params_rid),Lasso(random_state=0, **max_params_las),ElasticNet(random_state=0, **max_params_ela),\n",
    "             ARDRegression(**max_params_ard),BayesianRidge(**max_params_bay),LGBMRegressor(random_state=0,**max_params_lgbm),CatBoostRegressor(random_state=0)]\n",
    "regs_tuned = [(str(reg).split('(')[0], reg) for reg in regs_tuned]\n",
    "regs_tuned[-1] = list(regs_tuned[-1])\n",
    "regs_tuned[-1][0] = 'CatBoostRegressor'\n",
    "regs_tuned[-1] = tuple(regs_tuned[-1])\n",
    "\n",
    "regs_trained = [(name, reg.fit(x_train2, y_train2),mean_squared_error(reg.predict(x_dev),y_dev,squared=False))\n",
    "                    for name, reg in tqdm(regs_tuned.copy())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf7b34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "regs_tuned = [Ridge(random_state=0, **max_params_rid),Lasso(random_state=0, **max_params_las),ElasticNet(random_state=0, **max_params_ela),\n",
    "             ARDRegression(**max_params_ard),BayesianRidge(**max_params_bay),LGBMRegressor(random_state=0,**max_params_lgbm),CatBoostRegressor(random_state=0)]\n",
    "regs_tuned = [(str(reg).split('(')[0], reg) for reg in regs_tuned]\n",
    "regs_tuned[-1] = list(regs_tuned[-1])\n",
    "regs_tuned[-1][0] = 'CatBoostRegressor'\n",
    "regs_tuned[-1] = tuple(regs_tuned[-1])\n",
    "\n",
    "regs_trained_for_submissions = [(name, reg.fit(x_train,y_train)) for name, reg in tqdm(regs_tuned.copy())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11e832b",
   "metadata": {},
   "outputs": [],
   "source": [
    "regs_tuned = [Ridge(random_state=0, **max_params_rid),Lasso(random_state=0, **max_params_las),ElasticNet(random_state=0, **max_params_ela),\n",
    "             ARDRegression(**max_params_ard),BayesianRidge(**max_params_bay),LGBMRegressor(random_state=0,**max_params_lgbm),CatBoostRegressor(random_state=0)]\n",
    "regs_tuned = [(str(reg).split('(')[0], reg) for reg in regs_tuned]\n",
    "regs_tuned[-1] = list(regs_tuned[-1])\n",
    "regs_tuned[-1][0] = 'CatBoostRegressor'\n",
    "regs_tuned[-1] = tuple(regs_tuned[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd6b24c",
   "metadata": {},
   "source": [
    "### Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc5323c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_seeds(reset_graph_with_backend=None):\n",
    "    if reset_graph_with_backend is not None:\n",
    "        K = reset_graph_with_backend\n",
    "        K.clear_session()\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        print(\"KERAS AND TENSORFLOW GRAPHS RESET\")  # optional\n",
    "\n",
    "    np.random.seed(1)\n",
    "    # seed를 잘 설정하면 성능이 더 잘 오른다.\n",
    "    random.seed(2)\n",
    "    tf.compat.v1.set_random_seed(3)\n",
    "#    os.environ['CUDA_VISIBLE_DEVICES'] = ''  # for GPU\n",
    "    print(\"RANDOM SEEDS RESET\")  # optional\n",
    "   \n",
    "reset_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122d5c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = int(round(x_train2.shape[0] * 0.8,0))\n",
    "x_val, y_val = x_train2[i:], y_train2[i:]\n",
    "x_train3, y_train3 = x_train2[:i], y_train2[:i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62fe5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(hp):\n",
    "    inputs = tf.keras.Input(shape=(x_train3.shape[1],))\n",
    "    x = inputs\n",
    "    for i in range(hp.Int('num_layers', 2, 4, step=1)):\n",
    "        x = tf.keras.layers.Dense(hp.Int('unit_'+str(i), 16, 128, step=16),\n",
    "                               activation=hp.Choice('activation',['relu','tanh']))(x)\n",
    "        x = tf.keras.layers.Dropout(hp.Float('dropout_'+str(i), 0, 0.5, step=0.25, default=0.5))(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(loss='mse', \n",
    "                  optimizer=tf.keras.optimizers.Adam(hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])), \n",
    "                  metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572060d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras tuner는 튜닝 종류가 4종류가 있음: hyperband, grid search, random search, bayesian optimization\n",
    "tuner = kt.Hyperband(model_fn,\n",
    "                     objective=kt.Objective('val_root_mean_squared_error', direction=\"min\"), \n",
    "                     max_epochs=30,\n",
    "                     hyperband_iterations=2,\n",
    "                     overwrite=True,\n",
    "                     directory='dnn_tuning')\n",
    "# objective: 튜닝 기준, hyperband_iterations:이거 자체에서 2번 반복\n",
    "# overwrite: False시, 기존을 근거로 해 재학습 안시킴\n",
    "\n",
    "tuner.search(x_train3, y_train3, validation_data=(x_val, y_val),\n",
    "             callbacks=[tf.keras.callbacks.EarlyStopping()])\n",
    "# 빨리 끝내려고 파라미터 저렇게 설정한 것임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aa2019",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary(1) # 1= 제일 성능이 좋은 놈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b572f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss & RMSE\n",
    "dnn = tuner.get_best_models(1)[0] # best model 중 가장 좋은 모델\n",
    "dnn.evaluate(x_dev, y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835a9f43",
   "metadata": {},
   "source": [
    "### Model Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82e583a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results = []\n",
    "for name, reg, reg_score in regs_trained:\n",
    "    pred = list(reg.predict(x_dev))\n",
    "    name = f'{name} \\n({reg_score:.4f})'\n",
    "    pred_results.append(pd.Series(pred, name=name))\n",
    "ensemble_results = pd.concat(pred_results, axis=1)\n",
    "ensemble_results = ensemble_results.applymap(lambda x: float(x))\n",
    "\n",
    "# 모형의 예측값 간의 상관관계를 보기 위해 hitmap을 도식한다.\n",
    "plt.figure(figsize = (8,6))\n",
    "g = sns.heatmap(ensemble_results.corr(), annot=True, cmap='Blues')\n",
    "g.set_title(\"Correlation between models\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e3880a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = (ensemble_results.corr().sum()-1)/(ensemble_results.corr().shape[0]-1)\n",
    "names = corr.index\n",
    "rmse = np.array(corr.index.str[-7:-1]).astype(float)\n",
    "df = pd.DataFrame({'model': names, 'rmse': rmse, 'cor': corr})        \n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "g = sns.scatterplot(x=\"cor\", y=\"rmse\", data=df, s=40, color='red')\n",
    "for line in range(0, df.shape[0]):\n",
    "     g.text(df.cor[line]+0.003, df.rmse[line]-0.003, \n",
    "            df.model[line], horizontalalignment='left', \n",
    "            size='medium', color='black', weight='semibold')\n",
    "        \n",
    "plt.xlim((df.cor.min()-0.01,df.cor.max()+0.01))\n",
    "plt.ylim((df.rmse.min()-0.01,df.rmse.max()+0.01))\n",
    "plt.xlabel('Mean Agreement')\n",
    "plt.ylabel('RMSE')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43edfac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = [#'LinearRegression',\n",
    "            'Ridge',\n",
    "            #'Lasso',\n",
    "            #'ElasticNet',\n",
    "            #'ARDRegression',\n",
    "            #'BayesianRidge',\n",
    "            #'RandomForestRegressor',\n",
    "            #'XGBRegressor',\n",
    "            'LGBMRegressor',\n",
    "            'CatBoostRegressor'\n",
    "            ]\n",
    "models_for_ensemble = [(name,reg) for name,reg,score in regs_trained if name in selected]\n",
    "avg = (models_for_ensemble[0][1].predict(x_dev).flatten()+models_for_ensemble[1][1].predict(x_dev)+models_for_ensemble[2][1].predict(x_dev))/len(models_for_ensemble)\n",
    "score = mean_squared_error(avg, y_dev, squared=False)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc5d99c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 최적의 가중치 찾기 \n",
    "selected = [#'LinearRegression',\n",
    "            'Ridge',\n",
    "            #'Lasso',\n",
    "            #'ElasticNet',\n",
    "            #'ARDRegression',\n",
    "            #'BayesianRidge',\n",
    "            #'RandomForestRegressor',\n",
    "            #'XGBRegressor',\n",
    "            'LGBMRegressor',\n",
    "            'CatBoostRegressor',\n",
    "            #'DeepNeuralNetwork'\n",
    "            ]\n",
    "models_for_ensemble = [(name,reg) for name,reg,score in regs_trained if name in selected]\n",
    "weights_avg = []\n",
    "rmse_best = 1000\n",
    "for i in tqdm(range(1, 30, 1)):\n",
    "    for j in range(1, 30, 1):\n",
    "        for k in range(1, 30, 1):\n",
    "            if (i+j+k) != 30:\n",
    "                continue\n",
    "            pred = (models_for_ensemble[0][1].predict(x_dev).flatten() * i + models_for_ensemble[1][1].predict(x_dev) * j\n",
    "                    + models_for_ensemble[2][1].predict(x_dev) * k)/30\n",
    "            rmse = np.sqrt(mean_squared_error(y_dev, pred))\n",
    "            if rmse < rmse_best:\n",
    "                weights_avg = [i,j,k]\n",
    "                rmse_best = rmse \n",
    "                print(rmse, i,j,k)            \n",
    "\n",
    "print(rmse_best, weights_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8901cac5",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf733441",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = [#'LinearRegression',\n",
    "            'Ridge',\n",
    "            #'Lasso',\n",
    "            #'ElasticNet',\n",
    "            #'ARDRegression',\n",
    "            #'BayesianRidge',\n",
    "            #'RandomForestRegressor',\n",
    "            #'XGBRegressor',\n",
    "            'LGBMRegressor',\n",
    "            'CatBoostRegressor'\n",
    "            ]\n",
    "stack_estimators = [reg for name,reg,score in regs_trained if name in selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a0cedd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "S_train, S_test = stacking(stack_estimators,\n",
    "                           x_train, y_train, x_test,\n",
    "                           regression=True, n_folds=5, stratified=True, shuffle=True,\n",
    "                           random_state=0, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14457027",
   "metadata": {},
   "outputs": [],
   "source": [
    "S_train2, S_dev = stacking(stack_estimators,\n",
    "                           x_train2, y_train2, x_dev,\n",
    "                           regression=True, n_folds=5, stratified=True, shuffle=True,\n",
    "                           random_state=0, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b64372",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pbounds = {\n",
    "    'alpha':(0,150)\n",
    "}\n",
    "def rid_stk_opt(alpha):\n",
    "    params = {\n",
    "        'alpha':alpha\n",
    "    }\n",
    "    rid = Ridge(random_state=0, **params)\n",
    "    skf = StratifiedKFold(n_splits=5 , shuffle=True, random_state=1)\n",
    "    score = cross_val_score(rid, S_train2, y_train2, scoring='neg_mean_squared_error', cv=skf, n_jobs=-1)\n",
    "    return -np.sqrt(-np.mean(score))\n",
    "BO_stk_rid = BayesianOptimization(rid_stk_opt, pbounds, random_state=0)\n",
    "BO_stk_rid.maximize(init_points=50, n_iter=50) # init_points: exploration, n_iter: iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a759f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pbounds = {\n",
    "    'alpha':(0,50)\n",
    "}\n",
    "def las_stk_opt(alpha):\n",
    "    params = {\n",
    "        'alpha':alpha\n",
    "    }\n",
    "    las = Lasso(random_state=0, **params)\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    score = cross_val_score(las,S_train2, y_train2, scoring='neg_mean_squared_error', cv=skf, n_jobs=-1)\n",
    "    return -np.sqrt(-np.mean(score))\n",
    "BO_stk_las = BayesianOptimization(las_stk_opt, pbounds, random_state=0)\n",
    "BO_stk_las.maximize(init_points=50, n_iter=50) # init_points: exploration, n_iter: iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc07040f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pbounds = {\n",
    "    'alpha':(0,50)\n",
    "}\n",
    "def ela_stk_opt(alpha):\n",
    "    params = {\n",
    "        'alpha':alpha\n",
    "    }\n",
    "    ela = ElasticNet(random_state=0, **params)\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    score = cross_val_score(ela, S_train2, y_train2, scoring='neg_mean_squared_error', cv=skf, n_jobs=-1)\n",
    "    return -np.sqrt(-np.mean(score))\n",
    "BO_stk_ela = BayesianOptimization(ela_stk_opt, pbounds, random_state=0)\n",
    "BO_stk_ela.maximize(init_points=50, n_iter=50) # init_points: exploration, n_iter: iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5547f00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pbounds = {\n",
    "    'n_iter':(100,1000),\n",
    "    'alpha_1':(0,50),\n",
    "    'alpha_2':(0,50),\n",
    "    'lambda_1':(0,10),\n",
    "    'lambda_2':(0,10)\n",
    "}\n",
    "def ard_stk_opt(n_iter,alpha_1,alpha_2,lambda_1,lambda_2):\n",
    "    params = {\n",
    "        'n_iter':int(round(n_iter)),\n",
    "        'alpha_1':alpha_1,\n",
    "        'alpha_2':alpha_2,\n",
    "        'lambda_1':lambda_1,\n",
    "        'lambda_2':lambda_2\n",
    "    }\n",
    "    ard = ARDRegression(**params)\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    score = cross_val_score(ard, S_train2, y_train2, scoring='neg_mean_squared_error', cv=skf, n_jobs=-1)\n",
    "    return -np.sqrt(-np.mean(score))\n",
    "BO_stk_ard = BayesianOptimization(ard_stk_opt, pbounds, random_state=0)\n",
    "BO_stk_ard.maximize(init_points=50, n_iter=50) # init_points: exploration, n_iter: iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c180e35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pbounds = {\n",
    "    'n_iter':(100,1000),\n",
    "    'alpha_1':(0,50),\n",
    "    'alpha_2':(0,50),\n",
    "    'lambda_1':(0,10),\n",
    "    'lambda_2':(0,10)\n",
    "}\n",
    "def bay_stk_opt(n_iter,alpha_1,alpha_2,lambda_1,lambda_2):\n",
    "    params = {\n",
    "        'n_iter':int(round(n_iter)),\n",
    "        'alpha_1':alpha_1,\n",
    "        'alpha_2':alpha_2,\n",
    "        'lambda_1':lambda_1,\n",
    "        'lambda_2':lambda_2\n",
    "    }\n",
    "    bay = BayesianRidge(**params)\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    score = cross_val_score(bay,S_train2, y_train2, scoring='neg_mean_squared_error', cv=skf, n_jobs=-1)\n",
    "    return -np.sqrt(-np.mean(score))\n",
    "BO_stk_bay = BayesianOptimization(bay_stk_opt, pbounds, random_state=0)\n",
    "BO_stk_bay.maximize(init_points=50, n_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59274b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbounds = {\n",
    "    'n_estimators':(100,1000),\n",
    "    'learning_rate':(0.0000000000000000001,1),\n",
    "    'max_depth':(2, 32),\n",
    "    'num_leaves':(2, 64),\n",
    "    'min_child_samples':(10, 200),\n",
    "    'min_child_weight':(1, 50),\n",
    "    'subsample':(0.5, 1),\n",
    "    'colsample_bytree':(0.5, 1),\n",
    "    'max_bin':(10, 500),\n",
    "    'reg_lambda':(0.001, 10),\n",
    "    'reg_alpha':(0.01, 50)\n",
    "}\n",
    "def lgbm_stk_opt(n_estimators, learning_rate, max_depth, num_leaves, min_child_samples, min_child_weight,\n",
    "             subsample, colsample_bytree, max_bin, reg_lambda, reg_alpha):\n",
    "    params = {\n",
    "        \"n_estimators\":int(round(n_estimators)), \n",
    "        \"learning_rate\":learning_rate,\n",
    "        'max_depth':int(round(max_depth)),\n",
    "        'num_leaves':int(round(num_leaves)),\n",
    "        'min_child_samples': int(round(min_child_samples)),\n",
    "        'min_child_weight': int(round(min_child_weight)),\n",
    "        'subsample':max(min(subsample, 1), 0),\n",
    "        'colsample_bytree':max(min(colsample_bytree, 1), 0),\n",
    "        'reg_lambda': reg_lambda,\n",
    "        'reg_alpha': reg_alpha\n",
    "    }\n",
    "    lgbm = LGBMRegressor(random_state=0, **params)\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    score = cross_val_score(lgbm, S_train2, y_train2, scoring='neg_mean_squared_error', cv=skf, n_jobs=-1)\n",
    "    return -np.sqrt(-np.mean(score))\n",
    "BO_stk_lgbm = BayesianOptimization(lgbm_stk_opt, pbounds, random_state=0)\n",
    "BO_stk_lgbm.maximize(init_points=50, n_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d21024f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_params_rid = BO_stk_rid.max['params']\n",
    "max_params_las = BO_stk_las.max['params']\n",
    "max_params_ela = BO_stk_ela.max['params']\n",
    "max_params_ard = BO_stk_ard.max['params']\n",
    "max_params_bay = BO_stk_bay.max['params']\n",
    "max_params_lgbm = BO_stk_lgbm.max['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faceca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_params_ard['n_iter'] = int(round(max_params_ard['n_iter']))\n",
    "\n",
    "max_params_bay['n_iter'] = int(round(max_params_bay['n_iter']))\n",
    "\n",
    "max_params_lgbm['num_leaves'] = int(round(max_params_lgbm['num_leaves']))\n",
    "max_params_lgbm['n_estimators'] = int(round(max_params_lgbm['n_estimators']))\n",
    "max_params_lgbm['max_depth'] = int(round(max_params_lgbm['max_depth']))\n",
    "max_params_lgbm['min_child_samples'] = int(round(max_params_lgbm['min_child_samples']))\n",
    "max_params_lgbm['min_child_weight'] = int(round(max_params_lgbm['min_child_weight']))\n",
    "max_params_lgbm['max_bin'] = int(round(max_params_lgbm['max_bin']))\n",
    "max_params_lgbm['subsample'] = max(min(max_params_lgbm['subsample'], 1), 0)\n",
    "max_params_lgbm['colsample_bytree'] = max(min(max_params_lgbm['colsample_bytree'], 1), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5bf8f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(max_params_rid,'\\n',max_params_las,'\\n',max_params_ela,'\\n',max_params_ard,'\\n',max_params_bay,'\\n',max_params_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08437853",
   "metadata": {},
   "outputs": [],
   "source": [
    "stks_tuned = [Ridge(random_state=0, **max_params_rid),Lasso(random_state=0, **max_params_las),ElasticNet(random_state=0, **max_params_ela),\n",
    "             ARDRegression(**max_params_ard),BayesianRidge(**max_params_bay),LGBMRegressor(random_state=0,**max_params_lgbm),CatBoostRegressor(random_state=0)]\n",
    "stks_tuned = [(str(stk).split('(')[0], stk) for stk in stks_tuned]\n",
    "stks_tuned[-1] = list(stks_tuned[-1])\n",
    "stks_tuned[-1][0] = 'CatBoostRegressor'\n",
    "stks_tuned[-1] = tuple(stks_tuned[-1])\n",
    "\n",
    "stks_trained = [(name, stk.fit(S_train2, y_train2),mean_squared_error(stk.predict(S_dev),y_dev,squared=False))\n",
    "                    for name, stk in tqdm(stks_tuned.copy())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f91a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stks_tuned = [Ridge(random_state=0, **max_params_rid),Lasso(random_state=0, **max_params_las),ElasticNet(random_state=0, **max_params_ela),\n",
    "             ARDRegression(**max_params_ard),BayesianRidge(**max_params_bay),LGBMRegressor(random_state=0,**max_params_lgbm),CatBoostRegressor(random_state=0)]\n",
    "stks_tuned = [(str(stk).split('(')[0], stk) for stk in stks_tuned]\n",
    "stks_tuned[-1] = list(stks_tuned[-1])\n",
    "stks_tuned[-1][0] = 'CatBoostRegressor'\n",
    "stks_tuned[-1] = tuple(stks_tuned[-1])\n",
    "\n",
    "stks_trained_for_submissions = [(name, stk.fit(S_train,y_train)) for name, stk in tqdm(stks_tuned.copy())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77e7342",
   "metadata": {},
   "outputs": [],
   "source": [
    "stks_tuned = [Ridge(random_state=0, **max_params_rid),Lasso(random_state=0, **max_params_las),ElasticNet(random_state=0, **max_params_ela),\n",
    "             ARDRegression(**max_params_ard),BayesianRidge(**max_params_bay),LGBMRegressor(random_state=0,**max_params_lgbm),CatBoostRegressor(random_state=0)]\n",
    "stks_tuned = [(str(stk).split('(')[0], stk) for stk in stks_tuned]\n",
    "stks_tuned[-1] = list(stks_tuned[-1])\n",
    "stks_tuned[-1][0] = 'CatBoostRegressor'\n",
    "stks_tuned[-1] = tuple(stks_tuned[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87ed2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results = []\n",
    "for name, stk, stk_score in stks_trained:\n",
    "    pred = list(stk.predict(S_dev))\n",
    "    name = f'{name} \\n({stk_score:.4f})'\n",
    "    pred_results.append(pd.Series(pred, name=name))\n",
    "ensemble_results = pd.concat(pred_results, axis=1)\n",
    "ensemble_results = ensemble_results.applymap(lambda x: float(x))\n",
    "\n",
    "# 모형의 예측값 간의 상관관계를 보기 위해 hitmap을 도식한다.\n",
    "plt.figure(figsize = (8,6))\n",
    "g = sns.heatmap(ensemble_results.corr(), annot=True, cmap='Blues')\n",
    "g.set_title(\"Correlation between models\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4313497a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = (ensemble_results.corr().sum()-1)/(ensemble_results.corr().shape[0]-1)\n",
    "names = corr.index\n",
    "rmse = np.array(corr.index.str[-7:-1]).astype(float)\n",
    "df = pd.DataFrame({'model': names, 'rmse': rmse, 'cor': corr})        \n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "g = sns.scatterplot(x=\"cor\", y=\"rmse\", data=df, s=40, color='red')\n",
    "for line in range(0, df.shape[0]):\n",
    "     g.text(df.cor[line]+0.003, df.rmse[line]-0.003, \n",
    "            df.model[line], horizontalalignment='left', \n",
    "            size='medium', color='black', weight='semibold')\n",
    "        \n",
    "plt.xlim((df.cor.min()-0.01,df.cor.max()+0.01))\n",
    "plt.ylim((df.rmse.min()-0.01,df.rmse.max()+0.01))\n",
    "plt.xlabel('Mean Agreement')\n",
    "plt.ylabel('RMSE')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d17b00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selected = [#'LinearRegression',\n",
    "            'Ridge',\n",
    "            #'Lasso',\n",
    "            #'ElasticNet',\n",
    "            #'ARDRegression',\n",
    "            #'BayesianRidge',\n",
    "            #'RandomForestRegressor',\n",
    "            #'XGBRegressor',\n",
    "            'LGBMRegressor',\n",
    "            'CatBoostRegressor',\n",
    "            #'DeepNeuralNetwork'\n",
    "            ]\n",
    "models_for_ensemble = [(name,reg) for name,reg,score in stks_trained if name in selected]\n",
    "results_for_ensemble = []\n",
    "weights_stk = []\n",
    "rmse_best = 1000\n",
    "for i in tqdm(range(1, 90, 1)):\n",
    "    for j in range(1, 90, 1):\n",
    "        for k in range(1, 90, 1):\n",
    "            if (i+j+k) != 90:\n",
    "                continue\n",
    "            pred = (models_for_ensemble[0][1].predict(S_dev).flatten() * i + models_for_ensemble[1][1].predict(S_dev) * j\n",
    "                    + models_for_ensemble[2][1].predict(S_dev) * k)/90\n",
    "            rmse = np.sqrt(mean_squared_error(y_dev, pred))\n",
    "            if rmse < rmse_best:\n",
    "                weights_stk = [i,j,k]\n",
    "                rmse_best = rmse \n",
    "                print(rmse, i,j,k)            \n",
    "\n",
    "print(rmse_best, weights_stk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cf325f",
   "metadata": {},
   "source": [
    "3-layered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d048e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = [#'LinearRegression',\n",
    "            'Ridge',\n",
    "            #'Lasso',\n",
    "            #'ElasticNet',\n",
    "            #'ARDRegression',\n",
    "            #'BayesianRidge',\n",
    "            #'RandomForestRegressor',\n",
    "            #'XGBRegressor',\n",
    "            'LGBMRegressor',\n",
    "            'CatBoostRegressor'\n",
    "            ]\n",
    "tri_stack_estimators = [reg for name,reg,score in stks_trained if name in selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78caf85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "S3_train, S3_test = stacking(tri_stack_estimators,\n",
    "                           x_train, y_train, x_test,\n",
    "                           regression=True, n_folds=5, stratified=True, shuffle=True,\n",
    "                           random_state=0, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b956fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_train2, S3_dev = stacking(tri_stack_estimators,\n",
    "                           x_train2, y_train2, x_dev,\n",
    "                           regression=True, n_folds=5, stratified=True, shuffle=True,\n",
    "                           random_state=0, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e905751a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pbounds = {\n",
    "    'alpha':(0,150)\n",
    "}\n",
    "def rid_tri_stk_opt(alpha):\n",
    "    params = {\n",
    "        'alpha':alpha\n",
    "    }\n",
    "    rid = Ridge(random_state=0, **params)\n",
    "    skf = StratifiedKFold(n_splits=5 , shuffle=True, random_state=1)\n",
    "    score = cross_val_score(rid, S3_train2, y_train2, scoring='neg_mean_squared_error', cv=skf, n_jobs=-1)\n",
    "    return -np.sqrt(-np.mean(score))\n",
    "BO_stk_rid = BayesianOptimization(rid_tri_stk_opt, pbounds, random_state=0)\n",
    "BO_stk_rid.maximize(init_points=50, n_iter=50) # init_points: exploration, n_iter: iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088f6987",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pbounds = {\n",
    "    'alpha':(0,50)\n",
    "}\n",
    "def las_tri_stk_opt(alpha):\n",
    "    params = {\n",
    "        'alpha':alpha\n",
    "    }\n",
    "    las = Lasso(random_state=0, **params)\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    score = cross_val_score(las,S3_train2, y_train2, scoring='neg_mean_squared_error', cv=skf, n_jobs=-1)\n",
    "    return -np.sqrt(-np.mean(score))\n",
    "BO_stk_las = BayesianOptimization(las_tri_stk_opt, pbounds, random_state=0)\n",
    "BO_stk_las.maximize(init_points=50, n_iter=50) # init_points: exploration, n_iter: iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fbdd59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pbounds = {\n",
    "    'alpha':(0,50)\n",
    "}\n",
    "def ela_tri_stk_opt(alpha):\n",
    "    params = {\n",
    "        'alpha':alpha\n",
    "    }\n",
    "    ela = ElasticNet(random_state=0, **params)\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    score = cross_val_score(ela, S3_train2, y_train2, scoring='neg_mean_squared_error', cv=skf, n_jobs=-1)\n",
    "    return -np.sqrt(-np.mean(score))\n",
    "BO_stk_ela = BayesianOptimization(ela_tri_stk_opt, pbounds, random_state=0)\n",
    "BO_stk_ela.maximize(init_points=50, n_iter=50) # init_points: exploration, n_iter: iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd79649",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pbounds = {\n",
    "    'n_iter':(100,1000),\n",
    "    'alpha_1':(0,50),\n",
    "    'alpha_2':(0,50),\n",
    "    'lambda_1':(0,10),\n",
    "    'lambda_2':(0,10)\n",
    "}\n",
    "def ard_tri_stk_opt(n_iter,alpha_1,alpha_2,lambda_1,lambda_2):\n",
    "    params = {\n",
    "        'n_iter':int(round(n_iter)),\n",
    "        'alpha_1':alpha_1,\n",
    "        'alpha_2':alpha_2,\n",
    "        'lambda_1':lambda_1,\n",
    "        'lambda_2':lambda_2\n",
    "    }\n",
    "    ard = ARDRegression(**params)\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    score = cross_val_score(ard, S3_train2, y_train2, scoring='neg_mean_squared_error', cv=skf, n_jobs=-1)\n",
    "    return -np.sqrt(-np.mean(score))\n",
    "BO_stk_ard = BayesianOptimization(ard_tri_stk_opt, pbounds, random_state=0)\n",
    "BO_stk_ard.maximize(init_points=50, n_iter=50) # init_points: exploration, n_iter: iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d1f636",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pbounds = {\n",
    "    'n_iter':(100,1000),\n",
    "    'alpha_1':(0,50),\n",
    "    'alpha_2':(0,50),\n",
    "    'lambda_1':(0,10),\n",
    "    'lambda_2':(0,10)\n",
    "}\n",
    "def bay_tri_stk_opt(n_iter,alpha_1,alpha_2,lambda_1,lambda_2):\n",
    "    params = {\n",
    "        'n_iter':int(round(n_iter)),\n",
    "        'alpha_1':alpha_1,\n",
    "        'alpha_2':alpha_2,\n",
    "        'lambda_1':lambda_1,\n",
    "        'lambda_2':lambda_2\n",
    "    }\n",
    "    bay = BayesianRidge(**params)\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    score = cross_val_score(bay,S3_train2, y_train2, scoring='neg_mean_squared_error', cv=skf, n_jobs=-1)\n",
    "    return -np.sqrt(-np.mean(score))\n",
    "BO_stk_bay = BayesianOptimization(bay_tri_stk_opt, pbounds, random_state=0)\n",
    "BO_stk_bay.maximize(init_points=50, n_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2055366",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbounds = {\n",
    "    'n_estimators':(100,1000),\n",
    "    'learning_rate':(0.0000000000000000001,1),\n",
    "    'max_depth':(2, 32),\n",
    "    'num_leaves':(2, 64),\n",
    "    'min_child_samples':(10, 200),\n",
    "    'min_child_weight':(1, 50),\n",
    "    'subsample':(0.5, 1),\n",
    "    'colsample_bytree':(0.5, 1),\n",
    "    'max_bin':(10, 500),\n",
    "    'reg_lambda':(0.001, 10),\n",
    "    'reg_alpha':(0.01, 50)\n",
    "}\n",
    "def lgbm_tri_stk_opt(n_estimators, learning_rate, max_depth, num_leaves, min_child_samples, min_child_weight,\n",
    "             subsample, colsample_bytree, max_bin, reg_lambda, reg_alpha):\n",
    "    params = {\n",
    "        \"n_estimators\":int(round(n_estimators)), \n",
    "        \"learning_rate\":learning_rate,\n",
    "        'max_depth':int(round(max_depth)),\n",
    "        'num_leaves':int(round(num_leaves)),\n",
    "        'min_child_samples': int(round(min_child_samples)),\n",
    "        'min_child_weight': int(round(min_child_weight)),\n",
    "        'subsample':max(min(subsample, 1), 0),\n",
    "        'colsample_bytree':max(min(colsample_bytree, 1), 0),\n",
    "        'reg_lambda': reg_lambda,\n",
    "        'reg_alpha': reg_alpha\n",
    "    }\n",
    "    lgbm = LGBMRegressor(random_state=0, **params)\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    score = cross_val_score(lgbm, S3_train2, y_train2, scoring='neg_mean_squared_error', cv=skf, n_jobs=-1)\n",
    "    return -np.sqrt(-np.mean(score))\n",
    "BO_stk_lgbm = BayesianOptimization(lgbm_tri_stk_opt, pbounds, random_state=0)\n",
    "BO_stk_lgbm.maximize(init_points=50, n_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451a7d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_params_rid = BO_stk_rid.max['params']\n",
    "max_params_las = BO_stk_las.max['params']\n",
    "max_params_ela = BO_stk_ela.max['params']\n",
    "max_params_ard = BO_stk_ard.max['params']\n",
    "max_params_bay = BO_stk_bay.max['params']\n",
    "max_params_lgbm = BO_stk_lgbm.max['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be566cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_params_ard['n_iter'] = int(round(max_params_ard['n_iter']))\n",
    "\n",
    "max_params_bay['n_iter'] = int(round(max_params_bay['n_iter']))\n",
    "\n",
    "max_params_lgbm['num_leaves'] = int(round(max_params_lgbm['num_leaves']))\n",
    "max_params_lgbm['n_estimators'] = int(round(max_params_lgbm['n_estimators']))\n",
    "max_params_lgbm['max_depth'] = int(round(max_params_lgbm['max_depth']))\n",
    "max_params_lgbm['min_child_samples'] = int(round(max_params_lgbm['min_child_samples']))\n",
    "max_params_lgbm['min_child_weight'] = int(round(max_params_lgbm['min_child_weight']))\n",
    "max_params_lgbm['max_bin'] = int(round(max_params_lgbm['max_bin']))\n",
    "max_params_lgbm['subsample'] = max(min(max_params_lgbm['subsample'], 1), 0)\n",
    "max_params_lgbm['colsample_bytree'] = max(min(max_params_lgbm['colsample_bytree'], 1), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807f2bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_params_rid,'\\n',max_params_las,'\\n',max_params_ela,'\\n',max_params_ard,'\\n',max_params_bay,'\\n',max_params_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07aaa0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_stks_tuned = [Ridge(random_state=0, **max_params_rid),Lasso(random_state=0, **max_params_las),ElasticNet(random_state=0, **max_params_ela),\n",
    "             ARDRegression(**max_params_ard),BayesianRidge(**max_params_bay),LGBMRegressor(random_state=0,**max_params_lgbm),CatBoostRegressor(random_state=0)]\n",
    "tri_stks_tuned = [(str(stk).split('(')[0], stk) for stk in tri_stks_tuned]\n",
    "tri_stks_tuned[-1] = list(tri_stks_tuned[-1])\n",
    "tri_stks_tuned[-1][0] = 'CatBoostRegressor'\n",
    "tri_stks_tuned[-1] = tuple(tri_stks_tuned[-1])\n",
    "\n",
    "tri_stks_trained = [(name, stk.fit(S3_train2, y_train2),mean_squared_error(stk.predict(S3_dev),y_dev,squared=False))\n",
    "                    for name, stk in tqdm(tri_stks_tuned.copy())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4de81f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_stks_tuned = [Ridge(random_state=0, **max_params_rid),Lasso(random_state=0, **max_params_las),ElasticNet(random_state=0, **max_params_ela),\n",
    "             ARDRegression(**max_params_ard),BayesianRidge(**max_params_bay),LGBMRegressor(random_state=0,**max_params_lgbm),CatBoostRegressor(random_state=0)]\n",
    "tri_stks_tuned = [(str(stk).split('(')[0], stk) for stk in tri_stks_tuned]\n",
    "tri_stks_tuned[-1] = list(tri_stks_tuned[-1])\n",
    "tri_stks_tuned[-1][0] = 'CatBoostRegressor'\n",
    "tri_stks_tuned[-1] = tuple(tri_stks_tuned[-1])\n",
    "\n",
    "tri_stks_trained_for_submissions = [(name, stk.fit(S3_train,y_train)) for name, stk in tqdm(tri_stks_tuned.copy())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c78837",
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_stks_tuned = [Ridge(random_state=0, **max_params_rid),Lasso(random_state=0, **max_params_las),ElasticNet(random_state=0, **max_params_ela),\n",
    "             ARDRegression(**max_params_ard),BayesianRidge(**max_params_bay),LGBMRegressor(random_state=0,**max_params_lgbm),CatBoostRegressor(random_state=0)]\n",
    "tri_stks_tuned = [(str(stk).split('(')[0], stk) for stk in tri_stks_tuned]\n",
    "tri_stks_tuned[-1] = list(tri_stks_tuned[-1])\n",
    "tri_stks_tuned[-1][0] = 'CatBoostRegressor'\n",
    "tri_stks_tuned[-1] = tuple(tri_stks_tuned[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9520d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4f7540",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selected = [#'LinearRegression',\n",
    "            'Ridge',\n",
    "            #'Lasso',\n",
    "            #'ElasticNet',\n",
    "            #'ARDRegression',\n",
    "            #'BayesianRidge',\n",
    "            #'RandomForestRegressor',\n",
    "            #'XGBRegressor',\n",
    "            'LGBMRegressor',\n",
    "            'CatBoostRegressor',\n",
    "            #'DeepNeuralNetwork'\n",
    "            ]\n",
    "models_for_ensemble = [(name,reg) for name,reg,score in stks_trained if name in selected]\n",
    "w0,w1,w2 = weights_stk\n",
    "stk_avg = (models_for_ensemble[0][1].predict(S_dev).flatten()*w0+models_for_ensemble[1][1].predict(S_dev)*w1\n",
    "            + models_for_ensemble[2][1].predict(S_dev)*w2)/90\n",
    "weights_ad = []\n",
    "rmse_best = 1000\n",
    "for i in tqdm(range(1, 100, 1)):\n",
    "    for j in range(1, 100, 1):\n",
    "        if (i+j) != 100:\n",
    "            continue\n",
    "        pred = (stk_avg*i + dnn.predict(x_dev).flatten()*j)/100\n",
    "        rmse = np.sqrt(mean_squared_error(y_dev, pred))\n",
    "        if rmse < rmse_best:\n",
    "            weights_ad = [i,j]\n",
    "            rmse_best = rmse \n",
    "            print(rmse, i,j)            \n",
    "\n",
    "print(rmse_best, weights_ad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251b2e15",
   "metadata": {},
   "source": [
    "### Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7657e843",
   "metadata": {},
   "outputs": [],
   "source": [
    "w0,w1,w2 = weights_stk\n",
    "stk_avg = (models_for_ensemble[0][1].predict(S_test).flatten()*w0+models_for_ensemble[1][1].predict(S_test)*w1\n",
    "            + models_for_ensemble[2][1].predict(S_test)*w2)/90\n",
    "w0,w1 = weights_ad\n",
    "pred = (stk_avg*w0 + dnn.predict(x_test).flatten()*w1)/100\n",
    "pd.DataFrame({'custid': test_id, 'age':pred}).to_csv('averagingstk77_ridlgbmcat_dnn23.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
